{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6626fae3-de0a-48db-9225-6462a6a2e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import subprocess\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9af95ef7-55a3-4dd5-b4ed-249fd6566f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gzip: ./warc_paths/warc_2013.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2014.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2015.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2016.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2017.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2018.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2019.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2020.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2021.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2022.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2023.paths already exists;\tnot overwritten\n",
      "gzip: ./warc_paths/warc_2024.paths already exists;\tnot overwritten\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"warc_paths\", exist_ok=True)\n",
    "with open(\"yearly_crawls.txt\", 'r') as f:\n",
    "    for cc_crawl in f:\n",
    "        year = cc_crawl.split('-')[-2]\n",
    "        file_name = f\"./warc_paths/warc_{year}.paths.gz\"\n",
    "        urlretrieve(cc_crawl, file_name)\n",
    "        os.system(f\"gzip -d {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fd9a9e9-b0b3-4710-b3a6-05e08b9e170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_warcs_to_proc(wp_file: str) -> int:\n",
    "    \"\"\"Returns the number of lines in the warc.paths file.\"\"\"\n",
    "    with open(f\"./warc_paths/{wp_file}\", 'r') as f:\n",
    "        for count,_ in enumerate(f):\n",
    "            pass\n",
    "    return count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22a712fc-0cfc-4fcc-bb93-0e7c604ccdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_file_splits(wp_file: str):\n",
    "    \"\"\"Given a warc.paths file, generates `.txt` files having specified number of WARC filepaths\"\"\"\n",
    "    warc_sample_len = num_warcs_to_proc(wp_file) // 100\n",
    "    os.system(f\"./file_split.sh warc_paths/{wp_file} warc_splits/ {warc_sample_len} {wp_file.split('_')[-1].split('.')[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "410a7b8e-c71b-4bf5-b9b5-a61b90304e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paths(input_txt):\n",
    "    \"\"\"Converts the WARC URLs to their corresponding paths on the device.\"\"\"\n",
    "    updated = []\n",
    "    with open(input_txt, 'r') as f:\n",
    "        for l in f:\n",
    "            l = l.split('/')[-1]\n",
    "            updated.append(\"/opt/workspace/datasets/common_crawl/\" + '.'.join(l.split('.')[:-1]))\n",
    "\n",
    "    with open(input_txt, 'w') as f:\n",
    "        for l in updated:\n",
    "            f.write(l + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5aa8b0f-a747-4a15-bd54-bd99d4d8b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(input_txt: str):\n",
    "    \"\"\"Submits two spark jobs and waits for them to finish.\"\"\"\n",
    "    cmd1 = [\"spark-submit\", \"ipwarc_mmdb_pdudf.py\", \"--input_file\", f\"warc_splits/{input_txt}\", \"--output_dir\", \"ipmaxmind_out\"]\n",
    "    cmd2 = [\"spark-submit\", \"script_extraction.py\", \"--input_file\", f\"warc_splits/{input_txt}\", \"--output_dir\", \"script_extraction_out\"]\n",
    "\n",
    "    process1 = subprocess.Popen(cmd1)\n",
    "    process2 = subprocess.Popen(cmd2)\n",
    "\n",
    "    process1.wait()\n",
    "    process2.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9336162b-2511-4aa2-88b5-a9031f77e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wp(wp_file: str):\n",
    "    \"\"\"Process a warc.paths file by generating splits, and submitting each of the split `.txt` file to spark.\"\"\"\n",
    "    start_time = time.time()\n",
    "    os.makedirs(\"warc_splits\", exist_ok=True)\n",
    "    gen_file_splits(wp)\n",
    "    \n",
    "    ckpt_dir = pathlib.Path(\"warc_splits/.ipynb_checkpoints/\")\n",
    "    if ckpt_dir.exists() and ckpt_dir.is_dir():\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "    data_dir = \"/opt/workspace/datasets/common_crawl/\"\n",
    "    for input_txt in sorted(os.listdir(\"warc_splits\")):\n",
    "        os.makedirs(data_dir)\n",
    "        os.system(f\"./get_files.sh warc_splits/{input_txt} {data_dir}\")\n",
    "        to_paths(f\"warc_splits/{input_txt}\")\n",
    "        submit_job(input_txt)\n",
    "        shutil.rmtree(data_dir)\n",
    "        \n",
    "    shutil.rmtree(\"./warc_splits\")\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c990e1a5-5516-4f89-916e-d31fa7a31637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File split completed. Files saved in warc_splits/\n",
      "Downloading files from file: warc_splits/warc_part_001.txt ...\n",
      "Total files to download: 50\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640461318.24/warc/CC-MAIN-20240806001923-20240806031923-00708.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640476915.25/warc/CC-MAIN-20240806064139-20240806094139-00035.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641085898.84/warc/CC-MAIN-20240813204036-20240813234036-00442.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641333615.45/warc/CC-MAIN-20240816030812-20240816060812-00313.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641045630.75/warc/CC-MAIN-20240812155418-20240812185418-00075.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641299002.97/warc/CC-MAIN-20240815141847-20240815171847-00567.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640726723.42/warc/CC-MAIN-20240808093647-20240808123647-00058.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640389685.8/warc/CC-MAIN-20240804041019-20240804071019-00280.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640455981.23/warc/CC-MAIN-20240805211050-20240806001050-00329.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641036895.73/warc/CC-MAIN-20240812092946-20240812122946-00780.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640372747.5/warc/CC-MAIN-20240803153056-20240803183056-00764.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640762343.50/warc/CC-MAIN-20240809075530-20240809105530-00843.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640404969.12/warc/CC-MAIN-20240804133418-20240804163418-00210.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640484318.27/warc/CC-MAIN-20240806095414-20240806125414-00075.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640826253.62/warc/CC-MAIN-20240810221853-20240811011853-00248.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640361431.2/warc/CC-MAIN-20240803060126-20240803090126-00681.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641333615.45/warc/CC-MAIN-20240816030812-20240816060812-00223.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640805409.58/warc/CC-MAIN-20240810093040-20240810123040-00509.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640741453.47/warc/CC-MAIN-20240808222119-20240809012119-00429.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640368581.4/warc/CC-MAIN-20240803121937-20240803151937-00097.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641075627.80/warc/CC-MAIN-20240813075138-20240813105138-00516.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640736186.44/warc/CC-MAIN-20240808155812-20240808185812-00234.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641316011.99/warc/CC-MAIN-20240815204329-20240815234329-00285.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641319057.20/warc/CC-MAIN-20240815235528-20240816025528-00817.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640983659.65/warc/CC-MAIN-20240811075334-20240811105334-00829.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640826253.62/warc/CC-MAIN-20240810221853-20240811011853-00664.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640782288.54/warc/CC-MAIN-20240809235615-20240810025615-00654.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640682181.33/warc/CC-MAIN-20240807045851-20240807075851-00089.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641118845.90/warc/CC-MAIN-20240814155004-20240814185004-00008.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640826253.62/warc/CC-MAIN-20240810221853-20240811011853-00553.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640782288.54/warc/CC-MAIN-20240809235615-20240810025615-00040.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640759711.49/warc/CC-MAIN-20240809044241-20240809074241-00444.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640713269.38/warc/CC-MAIN-20240807205613-20240807235613-00222.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640805409.58/warc/CC-MAIN-20240810093040-20240810123040-00675.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640484318.27/warc/CC-MAIN-20240806095414-20240806125414-00110.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640377613.6/warc/CC-MAIN-20240803183820-20240803213820-00822.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641002566.67/warc/CC-MAIN-20240811141716-20240811171716-00308.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641076695.81/warc/CC-MAIN-20240813110333-20240813140333-00044.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640694449.36/warc/CC-MAIN-20240807111957-20240807141957-00028.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640372747.5/warc/CC-MAIN-20240803153056-20240803183056-00295.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641063659.79/warc/CC-MAIN-20240813043946-20240813073946-00723.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641082193.83/warc/CC-MAIN-20240813172835-20240813202835-00797.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640523737.31/warc/CC-MAIN-20240806224232-20240807014232-00162.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640694449.36/warc/CC-MAIN-20240807111957-20240807141957-00833.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640361431.2/warc/CC-MAIN-20240803060126-20240803090126-00652.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640377613.6/warc/CC-MAIN-20240803183820-20240803213820-00722.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640393185.10/warc/CC-MAIN-20240804071743-20240804101743-00318.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641045630.75/warc/CC-MAIN-20240812155418-20240812185418-00495.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640404969.12/warc/CC-MAIN-20240804133418-20240804163418-00832.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640694449.36/warc/CC-MAIN-20240807111957-20240807141957-00425.warc.gz ...\n",
      "Total files downloaded: 50\n",
      "Processing downloaded .gz files...\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240803060126-20240803090126-00652.warc.gz ...\n",
      "File size before extraction:\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240803060126-20240803090126-00681.warc.gz ...\n",
      "File size before extraction:\n",
      "809M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240803060126-20240803090126-00652.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240803121937-20240803151937-00097.warc.gz ...\n",
      "File size before extraction:\n",
      "799M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240803060126-20240803090126-00681.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240803153056-20240803183056-00295.warc.gz ...\n",
      "File size before extraction:\n",
      "826M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240803121937-20240803151937-00097.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240803153056-20240803183056-00764.warc.gz ...\n",
      "File size before extraction:\n",
      "821M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240803153056-20240803183056-00295.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240803183820-20240803213820-00722.warc.gz ...\n",
      "File size before extraction:\n",
      "804M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240803153056-20240803183056-00764.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240803183820-20240803213820-00822.warc.gz ...\n",
      "File size before extraction:\n",
      "831M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240803183820-20240803213820-00722.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240804041019-20240804071019-00280.warc.gz ...\n",
      "File size before extraction:\n",
      "844M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240803183820-20240803213820-00822.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240804071743-20240804101743-00318.warc.gz ...\n",
      "File size before extraction:\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240804133418-20240804163418-00210.warc.gz ...\n",
      "File size before extraction:\n",
      "838M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240804041019-20240804071019-00280.warc.gz\n",
      "803M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240804071743-20240804101743-00318.warc.gz\n",
      "808M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240804133418-20240804163418-00210.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240804133418-20240804163418-00832.warc.gz ...\n",
      "File size before extraction:\n",
      "812M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240804133418-20240804163418-00832.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240805211050-20240806001050-00329.warc.gz ...\n",
      "File size before extraction:\n",
      "821M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240805211050-20240806001050-00329.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240806001923-20240806031923-00708.warc.gz ...\n",
      "File size before extraction:\n",
      "800M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240806001923-20240806031923-00708.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240806064139-20240806094139-00035.warc.gz ...\n",
      "File size before extraction:\n",
      "830M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240806064139-20240806094139-00035.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240806095414-20240806125414-00075.warc.gz ...\n",
      "File size before extraction:\n",
      "815M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240806095414-20240806125414-00075.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240806095414-20240806125414-00110.warc.gz ...\n",
      "File size before extraction:\n",
      "799M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240806095414-20240806125414-00110.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240806224232-20240807014232-00162.warc.gz ...\n",
      "File size before extraction:\n",
      "793M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240806224232-20240807014232-00162.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240807045851-20240807075851-00089.warc.gz ...\n",
      "File size before extraction:\n",
      "806M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240807045851-20240807075851-00089.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240807111957-20240807141957-00028.warc.gz ...\n",
      "File size before extraction:\n",
      "814M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240807111957-20240807141957-00028.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240807111957-20240807141957-00425.warc.gz ...\n",
      "File size before extraction:\n",
      "822M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240807111957-20240807141957-00425.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240807111957-20240807141957-00833.warc.gz ...\n",
      "File size before extraction:\n",
      "811M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240807111957-20240807141957-00833.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240807205613-20240807235613-00222.warc.gz ...\n",
      "File size before extraction:\n",
      "813M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240807205613-20240807235613-00222.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240808093647-20240808123647-00058.warc.gz ...\n",
      "File size before extraction:\n",
      "819M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240808093647-20240808123647-00058.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240808155812-20240808185812-00234.warc.gz ...\n",
      "File size before extraction:\n",
      "831M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240808155812-20240808185812-00234.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240808222119-20240809012119-00429.warc.gz ...\n",
      "File size before extraction:\n",
      "801M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240808222119-20240809012119-00429.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240809044241-20240809074241-00444.warc.gz ...\n",
      "File size before extraction:\n",
      "794M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240809044241-20240809074241-00444.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240809075530-20240809105530-00843.warc.gz ...\n",
      "File size before extraction:\n",
      "815M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240809075530-20240809105530-00843.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240809235615-20240810025615-00040.warc.gz ...\n",
      "File size before extraction:\n",
      "797M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240809235615-20240810025615-00040.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240809235615-20240810025615-00654.warc.gz ...\n",
      "File size before extraction:\n",
      "803M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240809235615-20240810025615-00654.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240810093040-20240810123040-00509.warc.gz ...\n",
      "File size before extraction:\n",
      "830M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240810093040-20240810123040-00509.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240810093040-20240810123040-00675.warc.gz ...\n",
      "File size before extraction:\n",
      "796M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240810093040-20240810123040-00675.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240810221853-20240811011853-00248.warc.gz ...\n",
      "File size before extraction:\n",
      "798M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240810221853-20240811011853-00248.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240810221853-20240811011853-00553.warc.gz ...\n",
      "File size before extraction:\n",
      "793M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240810221853-20240811011853-00553.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240810221853-20240811011853-00664.warc.gz ...\n",
      "File size before extraction:\n",
      "800M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240810221853-20240811011853-00664.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240811075334-20240811105334-00829.warc.gz ...\n",
      "File size before extraction:\n",
      "803M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240811075334-20240811105334-00829.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240811141716-20240811171716-00308.warc.gz ...\n",
      "File size before extraction:\n",
      "823M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240811141716-20240811171716-00308.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240812092946-20240812122946-00780.warc.gz ...\n",
      "File size before extraction:\n",
      "790M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240812092946-20240812122946-00780.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240812155418-20240812185418-00075.warc.gz ...\n",
      "File size before extraction:\n",
      "811M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240812155418-20240812185418-00075.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240812155418-20240812185418-00495.warc.gz ...\n",
      "File size before extraction:\n",
      "810M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240812155418-20240812185418-00495.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240813043946-20240813073946-00723.warc.gz ...\n",
      "File size before extraction:\n",
      "794M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240813043946-20240813073946-00723.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240813075138-20240813105138-00516.warc.gz ...\n",
      "File size before extraction:\n",
      "787M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240813075138-20240813105138-00516.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240813110333-20240813140333-00044.warc.gz ...\n",
      "File size before extraction:\n",
      "784M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240813110333-20240813140333-00044.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240813172835-20240813202835-00797.warc.gz ...\n",
      "File size before extraction:\n",
      "819M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240813172835-20240813202835-00797.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240813204036-20240813234036-00442.warc.gz ...\n",
      "File size before extraction:\n",
      "806M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240813204036-20240813234036-00442.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240814155004-20240814185004-00008.warc.gz ...\n",
      "File size before extraction:\n",
      "802M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240814155004-20240814185004-00008.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240815141847-20240815171847-00567.warc.gz ...\n",
      "File size before extraction:\n",
      "822M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240815141847-20240815171847-00567.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240815204329-20240815234329-00285.warc.gz ...\n",
      "File size before extraction:\n",
      "796M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240815204329-20240815234329-00285.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240815235528-20240816025528-00817.warc.gz ...\n",
      "File size before extraction:\n",
      "799M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240815235528-20240816025528-00817.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240816030812-20240816060812-00223.warc.gz ...\n",
      "File size before extraction:\n",
      "783M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240816030812-20240816060812-00223.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20240816030812-20240816060812-00313.warc.gz ...\n",
      "File size before extraction:\n",
      "808M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20240816030812-20240816060812-00313.warc.gz\n",
      "Download and extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 10:11:14 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/09/07 10:11:14 INFO SparkContext: OS info Linux, 6.8.0-39-generic, amd64\n",
      "24/09/07 10:11:14 INFO SparkContext: Java version 11.0.24\n",
      "24/09/07 10:11:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/07 10:11:14 INFO ResourceUtils: ==============================================================\n",
      "24/09/07 10:11:14 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/07 10:11:14 INFO ResourceUtils: ==============================================================\n",
      "24/09/07 10:11:14 INFO SparkContext: Submitted application: script_extraction\n",
      "24/09/07 10:11:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/07 10:11:14 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "24/09/07 10:11:14 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/07 10:11:14 INFO SecurityManager: Changing view acls to: root\n",
      "24/09/07 10:11:14 INFO SecurityManager: Changing modify acls to: root\n",
      "24/09/07 10:11:14 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/07 10:11:14 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/07 10:11:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "24/09/07 10:11:14 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/09/07 10:11:14 INFO SparkContext: OS info Linux, 6.8.0-39-generic, amd64\n",
      "24/09/07 10:11:14 INFO SparkContext: Java version 11.0.24\n",
      "24/09/07 10:11:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/07 10:11:14 INFO Utils: Successfully started service 'sparkDriver' on port 46671.\n",
      "24/09/07 10:11:14 INFO ResourceUtils: ==============================================================\n",
      "24/09/07 10:11:14 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/07 10:11:14 INFO ResourceUtils: ==============================================================\n",
      "24/09/07 10:11:14 INFO SparkContext: Submitted application: maxmind-warc\n",
      "24/09/07 10:11:14 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/07 10:11:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/07 10:11:14 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "24/09/07 10:11:14 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/07 10:11:14 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/07 10:11:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/07 10:11:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/07 10:11:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/07 10:11:14 INFO SecurityManager: Changing view acls to: root\n",
      "24/09/07 10:11:14 INFO SecurityManager: Changing modify acls to: root\n",
      "24/09/07 10:11:14 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/07 10:11:14 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/07 10:11:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "24/09/07 10:11:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5d3b2b21-83b7-4281-924c-6a0047d0bfd0\n",
      "24/09/07 10:11:14 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "24/09/07 10:11:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/07 10:11:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/07 10:11:14 INFO Utils: Successfully started service 'sparkDriver' on port 32909.\n",
      "24/09/07 10:11:14 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/07 10:11:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/09/07 10:11:14 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/07 10:11:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/07 10:11:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/07 10:11:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/07 10:11:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0925e04d-14f7-43a2-8cb9-a2bfa78eb9b1\n",
      "24/09/07 10:11:14 INFO FairSchedulableBuilder: Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1\n",
      "24/09/07 10:11:14 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "24/09/07 10:11:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "24/09/07 10:11:14 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 15 ms (0 ms spent in bootstraps)\n",
      "24/09/07 10:11:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/07 10:11:14 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240907101114-0060\n",
      "24/09/07 10:11:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36115.\n",
      "24/09/07 10:11:14 INFO NettyBlockTransferService: Server created on fc5436c4b308:36115\n",
      "24/09/07 10:11:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/07 10:11:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fc5436c4b308, 36115, None)\n",
      "24/09/07 10:11:14 INFO BlockManagerMasterEndpoint: Registering block manager fc5436c4b308:36115 with 434.4 MiB RAM, BlockManagerId(driver, fc5436c4b308, 36115, None)\n",
      "24/09/07 10:11:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fc5436c4b308, 36115, None)\n",
      "24/09/07 10:11:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fc5436c4b308, 36115, None)\n",
      "24/09/07 10:11:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/07 10:11:14 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "24/09/07 10:11:14 INFO FairSchedulableBuilder: Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "24/09/07 10:11:14 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 13 ms (0 ms spent in bootstraps)\n",
      "24/09/07 10:11:14 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/spark-events/app-20240907101114-0060.inprogress\n",
      "24/09/07 10:11:14 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240907101114-0061\n",
      "24/09/07 10:11:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33387.\n",
      "24/09/07 10:11:14 INFO NettyBlockTransferService: Server created on fc5436c4b308:33387\n",
      "24/09/07 10:11:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/07 10:11:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fc5436c4b308, 33387, None)\n",
      "24/09/07 10:11:14 INFO BlockManagerMasterEndpoint: Registering block manager fc5436c4b308:33387 with 434.4 MiB RAM, BlockManagerId(driver, fc5436c4b308, 33387, None)\n",
      "24/09/07 10:11:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fc5436c4b308, 33387, None)\n",
      "24/09/07 10:11:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fc5436c4b308, 33387, None)\n",
      "24/09/07 10:11:14 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "24/09/07 10:11:14 INFO Utils: Using initial executors = 2, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "24/09/07 10:11:14 INFO ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/0 on worker-20240901052944-172.18.0.8-46473 (172.18.0.8:46473) with 1 core(s)\n",
      "24/09/07 10:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/0 on hostPort 172.18.0.8:46473 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/1 on worker-20240901052944-172.18.0.6-44801 (172.18.0.6:44801) with 1 core(s)\n",
      "24/09/07 10:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/1 on hostPort 172.18.0.6:44801 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/0 is now RUNNING\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/1 is now RUNNING\n",
      "24/09/07 10:11:14 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "24/09/07 10:11:14 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/spark-events/app-20240907101114-0061.inprogress\n",
      "24/09/07 10:11:14 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "24/09/07 10:11:14 INFO Utils: Using initial executors = 2, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "24/09/07 10:11:14 INFO ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0061/0 on worker-20240901052944-172.18.0.7-40861 (172.18.0.7:40861) with 1 core(s)\n",
      "24/09/07 10:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0061/0 on hostPort 172.18.0.7:40861 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0061/1 on worker-20240901052943-172.18.0.4-41363 (172.18.0.4:41363) with 1 core(s)\n",
      "24/09/07 10:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0061/1 on hostPort 172.18.0.4:41363 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0061/1 is now RUNNING\n",
      "24/09/07 10:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0061/0 is now RUNNING\n",
      "24/09/07 10:11:15 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "24/09/07 10:11:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 208.0 B, free 434.4 MiB)\n",
      "24/09/07 10:11:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 276.0 B, free 434.4 MiB)\n",
      "24/09/07 10:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fc5436c4b308:36115 (size: 276.0 B, free: 434.4 MiB)\n",
      "24/09/07 10:11:15 INFO SparkContext: Created broadcast 0 from broadcast at NativeMethodAccessorImpl.java:0\n",
      "24/09/07 10:11:15 INFO SparkContext: Added file ./ip_utils.py at spark://fc5436c4b308:32909/files/ip_utils.py with timestamp 1725703875171\n",
      "24/09/07 10:11:15 INFO Utils: Copying /opt/workspace/warc_yearly/ip_utils.py to /tmp/spark-9446eae2-e740-4899-9418-9355cd2f54a5/userFiles-f8e4d1f8-829e-425b-a2e8-3b77a3f7b291/ip_utils.py\n",
      "24/09/07 10:11:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/09/07 10:11:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/09/07 10:11:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fc5436c4b308:36115 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/09/07 10:11:15 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/09/07 10:11:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/09/07 10:11:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/09/07 10:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fc5436c4b308:33387 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/09/07 10:11:15 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/09/07 10:11:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/07 10:11:15 INFO SharedState: Warehouse path is 'file:/opt/workspace/warc_yearly/spark-warehouse'.\n",
      "24/09/07 10:11:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/07 10:11:15 INFO SharedState: Warehouse path is 'file:/opt/workspace/warc_yearly/spark-warehouse'.\n",
      "24/09/07 10:11:16 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:38142) with ID 1,  ResourceProfileId 0\n",
      "24/09/07 10:11:16 INFO ExecutorMonitor: New executor 1 has registered (new total is 1)\n",
      "24/09/07 10:11:16 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.8:57550) with ID 0,  ResourceProfileId 0\n",
      "24/09/07 10:11:16 INFO ExecutorMonitor: New executor 0 has registered (new total is 2)\n",
      "24/09/07 10:11:16 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:41087 with 2.2 GiB RAM, BlockManagerId(1, 172.18.0.6, 41087, None)\n",
      "24/09/07 10:11:16 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.8:44355 with 2.2 GiB RAM, BlockManagerId(0, 172.18.0.8, 44355, None)\n",
      "24/09/07 10:11:17 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:34222) with ID 1,  ResourceProfileId 0\n",
      "24/09/07 10:11:17 INFO ExecutorMonitor: New executor 1 has registered (new total is 1)\n",
      "24/09/07 10:11:17 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:35198) with ID 0,  ResourceProfileId 0\n",
      "24/09/07 10:11:17 INFO ExecutorMonitor: New executor 0 has registered (new total is 2)\n",
      "24/09/07 10:11:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 208.0 B, free 434.2 MiB)\n",
      "24/09/07 10:11:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 334.0 B, free 434.2 MiB)\n",
      "24/09/07 10:11:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fc5436c4b308:33387 (size: 334.0 B, free: 434.4 MiB)\n",
      "24/09/07 10:11:17 INFO SparkContext: Created broadcast 1 from broadcast at NativeMethodAccessorImpl.java:0\n",
      "24/09/07 10:11:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:43323 with 2.2 GiB RAM, BlockManagerId(1, 172.18.0.4, 43323, None)\n",
      "24/09/07 10:11:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:36067 with 2.2 GiB RAM, BlockManagerId(0, 172.18.0.7, 36067, None)\n",
      "24/09/07 10:11:17 INFO CodeGenerator: Code generated in 123.176489 ms\n",
      "24/09/07 10:11:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/09/07 10:11:17 INFO FileInputFormat: Total input files to process : 1\n",
      "24/09/07 10:11:17 INFO DAGScheduler: Registering RDD 3 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/09/07 10:11:17 INFO DAGScheduler: Registering RDD 12 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/09/07 10:11:17 INFO DAGScheduler: Got map stage job 0 (csv at NativeMethodAccessorImpl.java:0) with 10 output partitions\n",
      "24/09/07 10:11:17 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/09/07 10:11:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "24/09/07 10:11:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "24/09/07 10:11:17 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/07 10:11:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.3 KiB, free 434.1 MiB)\n",
      "24/09/07 10:11:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
      "24/09/07 10:11:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fc5436c4b308:36115 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/09/07 10:11:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/07 10:11:17 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/07 10:11:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/09/07 10:11:17 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool default\n",
      "24/09/07 10:11:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7685 bytes) \n",
      "24/09/07 10:11:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.6, executor 1, partition 1, PROCESS_LOCAL, 7685 bytes) \n",
      "24/09/07 10:11:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:41087 (size: 5.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.8:44355 (size: 5.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:17 INFO CodeGenerator: Code generated in 191.175757 ms\n",
      "24/09/07 10:11:18 INFO CodeGenerator: Code generated in 26.116893 ms\n",
      "24/09/07 10:11:18 INFO CodeGenerator: Code generated in 7.789983 ms\n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:41087 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:18 INFO FileInputFormat: Total input files to process : 1\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Registering RDD 3 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Got map stage job 0 (csv at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.8:44355 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.3 KiB, free 434.1 MiB)\n",
      "24/09/07 10:11:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fc5436c4b308:33387 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/09/07 10:11:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/07 10:11:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/09/07 10:11:18 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool default\n",
      "24/09/07 10:11:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7856 bytes) \n",
      "24/09/07 10:11:18 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.4, executor 1, partition 1, PROCESS_LOCAL, 7856 bytes) \n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.7:36067 (size: 5.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.4:43323 (size: 5.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.7:36067 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.4:43323 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:18 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1007 ms on 172.18.0.6 (executor 1) (1/2)\n",
      "24/09/07 10:11:18 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51001\n",
      "24/09/07 10:11:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1095 ms on 172.18.0.8 (executor 0) (2/2)\n",
      "24/09/07 10:11:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool default\n",
      "24/09/07 10:11:18 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 1.139 s\n",
      "24/09/07 10:11:18 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/07 10:11:18 INFO DAGScheduler: running: Set()\n",
      "24/09/07 10:11:18 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1)\n",
      "24/09/07 10:11:18 INFO DAGScheduler: failed: Set()\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/07 10:11:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 33.1 KiB, free 434.1 MiB)\n",
      "24/09/07 10:11:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 434.1 MiB)\n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fc5436c4b308:36115 (size: 16.5 KiB, free: 434.3 MiB)\n",
      "24/09/07 10:11:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/07 10:11:18 INFO DAGScheduler: Submitting 10 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "24/09/07 10:11:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 10 tasks resource profile 0\n",
      "24/09/07 10:11:18 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool default\n",
      "24/09/07 10:11:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7702 bytes) \n",
      "24/09/07 10:11:18 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 3) (172.18.0.6, executor 1, partition 4, NODE_LOCAL, 7702 bytes) \n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.6:41087 (size: 16.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.8:44355 (size: 16.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.8:44355 (size: 276.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:19 INFO BlockManagerInfo: Added broadcast_0_python on disk on 172.18.0.8:44355 (size: 98.0 B)\n",
      "24/09/07 10:11:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:41087 (size: 276.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.8:57550\n",
      "24/09/07 10:11:19 INFO BlockManagerInfo: Added broadcast_0_python on disk on 172.18.0.6:41087 (size: 98.0 B)\n",
      "24/09/07 10:11:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.6:38142\n",
      "24/09/07 10:11:19 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1099 ms on 172.18.0.4 (executor 1) (1/2)\n",
      "24/09/07 10:11:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1115 ms on 172.18.0.7 (executor 0) (2/2)\n",
      "24/09/07 10:11:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool default\n",
      "24/09/07 10:11:19 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58395\n",
      "24/09/07 10:11:19 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 1.157 s\n",
      "24/09/07 10:11:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/07 10:11:19 INFO DAGScheduler: running: Set()\n",
      "24/09/07 10:11:19 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1)\n",
      "24/09/07 10:11:19 INFO DAGScheduler: failed: Set()\n",
      "24/09/07 10:11:19 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/07 10:11:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 74.1 KiB, free 434.1 MiB)\n",
      "24/09/07 10:11:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 29.8 KiB, free 434.0 MiB)\n",
      "24/09/07 10:11:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fc5436c4b308:33387 (size: 29.8 KiB, free: 434.3 MiB)\n",
      "24/09/07 10:11:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/07 10:11:19 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/09/07 10:11:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0\n",
      "24/09/07 10:11:19 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool default\n",
      "24/09/07 10:11:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.18.0.4, executor 1, partition 0, NODE_LOCAL, 7873 bytes) \n",
      "24/09/07 10:11:19 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (172.18.0.7, executor 0, partition 1, NODE_LOCAL, 7873 bytes) \n",
      "24/09/07 10:11:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:36067 (size: 29.8 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.4:43323 (size: 29.8 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.4:34222\n",
      "24/09/07 10:11:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.7:35198\n",
      "24/09/07 10:11:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/2 on worker-20240901052944-172.18.0.9-41535 (172.18.0.9:41535) with 1 core(s)\n",
      "24/09/07 10:11:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/2 on hostPort 172.18.0.9:41535 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:19 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 3 for resource profile id: 0)\n",
      "24/09/07 10:11:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/2 is now RUNNING\n",
      "24/09/07 10:11:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0061/2 on worker-20240901052944-172.18.0.8-46473 (172.18.0.8:46473) with 1 core(s)\n",
      "24/09/07 10:11:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0061/2 on hostPort 172.18.0.8:46473 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:20 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 3 for resource profile id: 0)\n",
      "24/09/07 10:11:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0061/2 is now RUNNING\n",
      "24/09/07 10:11:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.7:36067 (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:20 INFO BlockManagerInfo: Added broadcast_1_python on disk on 172.18.0.7:36067 (size: 139.0 B)\n",
      "24/09/07 10:11:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.4:43323 (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:20 INFO BlockManagerInfo: Added broadcast_1_python on disk on 172.18.0.4:43323 (size: 139.0 B)\n",
      "24/09/07 10:11:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/3 on worker-20240901052944-172.18.0.6-44801 (172.18.0.6:44801) with 1 core(s)\n",
      "24/09/07 10:11:20 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 5 for resource profile id: 0)\n",
      "24/09/07 10:11:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/3 on hostPort 172.18.0.6:44801 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/4 on worker-20240901052944-172.18.0.7-40861 (172.18.0.7:40861) with 1 core(s)\n",
      "24/09/07 10:11:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/4 on hostPort 172.18.0.7:40861 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/4 is now RUNNING\n",
      "24/09/07 10:11:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/3 is now RUNNING\n",
      "24/09/07 10:11:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/5 on worker-20240901052943-172.18.0.4-41363 (172.18.0.4:41363) with 1 core(s)\n",
      "24/09/07 10:11:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/5 on hostPort 172.18.0.4:41363 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/6 on worker-20240901052944-172.18.0.9-41535 (172.18.0.9:41535) with 1 core(s)\n",
      "24/09/07 10:11:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/6 on hostPort 172.18.0.9:41535 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/7 on worker-20240901052944-172.18.0.8-46473 (172.18.0.8:46473) with 1 core(s)\n",
      "24/09/07 10:11:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/7 on hostPort 172.18.0.8:46473 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:21 INFO ExecutorAllocationManager: Requesting 4 new executors because tasks are backlogged (new desired total will be 9 for resource profile id: 0)\n",
      "24/09/07 10:11:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/8 on worker-20240901052944-172.18.0.6-44801 (172.18.0.6:44801) with 1 core(s)\n",
      "24/09/07 10:11:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/8 on hostPort 172.18.0.6:44801 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/5 is now RUNNING\n",
      "24/09/07 10:11:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/7 is now RUNNING\n",
      "24/09/07 10:11:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/8 is now RUNNING\n",
      "24/09/07 10:11:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/6 is now RUNNING\n",
      "24/09/07 10:11:22 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:47172) with ID 2,  ResourceProfileId 0\n",
      "24/09/07 10:11:22 INFO ExecutorMonitor: New executor 2 has registered (new total is 3)\n",
      "24/09/07 10:11:22 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:35531 with 2.2 GiB RAM, BlockManagerId(2, 172.18.0.9, 35531, None)\n",
      "24/09/07 10:11:22 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (172.18.0.9, executor 2, partition 2, PROCESS_LOCAL, 7702 bytes) \n",
      "24/09/07 10:11:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.9:35531 (size: 16.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240907101114-0060/9 on worker-20240901052944-172.18.0.7-40861 (172.18.0.7:40861) with 1 core(s)\n",
      "24/09/07 10:11:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20240907101114-0060/9 on hostPort 172.18.0.7:40861 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/07 10:11:22 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 10 for resource profile id: 0)\n",
      "24/09/07 10:11:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240907101114-0060/9 is now RUNNING\n",
      "24/09/07 10:11:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.8:52128) with ID 2,  ResourceProfileId 0\n",
      "24/09/07 10:11:23 INFO ExecutorMonitor: New executor 2 has registered (new total is 3)\n",
      "24/09/07 10:11:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.8:40219 with 2.2 GiB RAM, BlockManagerId(2, 172.18.0.8, 40219, None)\n",
      "24/09/07 10:11:23 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (172.18.0.8, executor 2, partition 2, ANY, 7873 bytes) \n",
      "24/09/07 10:11:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.8:40219 (size: 29.8 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:24 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:49438) with ID 4,  ResourceProfileId 0\n",
      "24/09/07 10:11:24 INFO ExecutorMonitor: New executor 4 has registered (new total is 4)\n",
      "24/09/07 10:11:24 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:39051 with 2.2 GiB RAM, BlockManagerId(4, 172.18.0.7, 39051, None)\n",
      "24/09/07 10:11:24 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 5) (172.18.0.7, executor 4, partition 3, PROCESS_LOCAL, 7702 bytes) \n",
      "24/09/07 10:11:24 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:51056) with ID 3,  ResourceProfileId 0\n",
      "24/09/07 10:11:24 INFO ExecutorMonitor: New executor 3 has registered (new total is 5)\n",
      "24/09/07 10:11:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:35531 (size: 276.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:24 INFO BlockManagerInfo: Added broadcast_0_python on disk on 172.18.0.9:35531 (size: 98.0 B)\n",
      "24/09/07 10:11:24 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:35773 with 2.2 GiB RAM, BlockManagerId(3, 172.18.0.6, 35773, None)\n",
      "24/09/07 10:11:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.9:47172\n",
      "24/09/07 10:11:24 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (172.18.0.6, executor 3, partition 5, NODE_LOCAL, 7702 bytes) \n",
      "24/09/07 10:11:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:39051 (size: 16.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.6:35773 (size: 16.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:25 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 7) (172.18.0.9, executor 2, partition 7, PROCESS_LOCAL, 7702 bytes) \n",
      "24/09/07 10:11:25 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 2950 ms on 172.18.0.9 (executor 2) (1/10)\n",
      "24/09/07 10:11:25 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 8) (172.18.0.9, executor 2, partition 8, PROCESS_LOCAL, 7702 bytes) \n",
      "24/09/07 10:11:25 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:54194) with ID 5,  ResourceProfileId 0\n",
      "24/09/07 10:11:25 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 7) in 191 ms on 172.18.0.9 (executor 2) (2/10)\n",
      "24/09/07 10:11:25 INFO ExecutorMonitor: New executor 5 has registered (new total is 6)\n",
      "24/09/07 10:11:25 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:40423 with 2.2 GiB RAM, BlockManagerId(5, 172.18.0.4, 40423, None)\n",
      "24/09/07 10:11:25 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 8) in 156 ms on 172.18.0.9 (executor 2) (3/10)\n",
      "24/09/07 10:11:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.8:52128\n",
      "24/09/07 10:11:25 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:47182) with ID 6,  ResourceProfileId 0\n",
      "24/09/07 10:11:25 INFO ExecutorMonitor: New executor 6 has registered (new total is 7)\n",
      "24/09/07 10:11:25 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:45057 with 2.2 GiB RAM, BlockManagerId(6, 172.18.0.9, 45057, None)\n",
      "24/09/07 10:11:26 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:51068) with ID 8,  ResourceProfileId 0\n",
      "24/09/07 10:11:26 INFO ExecutorMonitor: New executor 8 has registered (new total is 8)\n",
      "24/09/07 10:11:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:41301 with 2.2 GiB RAM, BlockManagerId(8, 172.18.0.6, 41301, None)\n",
      "24/09/07 10:11:26 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.8:49134) with ID 7,  ResourceProfileId 0\n",
      "24/09/07 10:11:26 INFO ExecutorMonitor: New executor 7 has registered (new total is 9)\n",
      "24/09/07 10:11:26 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 9) (172.18.0.6, executor 8, partition 6, NODE_LOCAL, 7702 bytes) \n",
      "24/09/07 10:11:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.8:35199 with 2.2 GiB RAM, BlockManagerId(7, 172.18.0.8, 35199, None)\n",
      "24/09/07 10:11:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.7:39051 (size: 276.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:26 INFO BlockManagerInfo: Added broadcast_0_python on disk on 172.18.0.7:39051 (size: 98.0 B)\n",
      "24/09/07 10:11:26 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 10) (172.18.0.8, executor 7, partition 1, NODE_LOCAL, 7702 bytes) \n",
      "24/09/07 10:11:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.7:49438\n",
      "24/09/07 10:11:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.6:41301 (size: 16.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:35773 (size: 276.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.8:35199 (size: 16.5 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:11:26 INFO BlockManagerInfo: Added broadcast_0_python on disk on 172.18.0.6:35773 (size: 98.0 B)\n",
      "24/09/07 10:11:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.6:51056\n",
      "24/09/07 10:11:26 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:49452) with ID 9,  ResourceProfileId 0\n",
      "24/09/07 10:11:26 INFO ExecutorMonitor: New executor 9 has registered (new total is 10)\n",
      "24/09/07 10:11:27 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 5) in 2687 ms on 172.18.0.7 (executor 4) (4/10)\n",
      "24/09/07 10:11:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:41809 with 2.2 GiB RAM, BlockManagerId(9, 172.18.0.7, 41809, None)\n",
      "24/09/07 10:11:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.8:40219 (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:27 INFO BlockManagerInfo: Added broadcast_1_python on disk on 172.18.0.8:40219 (size: 139.0 B)\n",
      "24/09/07 10:11:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:41301 (size: 276.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.8:35199 (size: 276.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:11:27 INFO BlockManagerInfo: Added broadcast_0_python on disk on 172.18.0.6:41301 (size: 98.0 B)\n",
      "24/09/07 10:11:27 INFO BlockManagerInfo: Added broadcast_0_python on disk on 172.18.0.8:35199 (size: 98.0 B)\n",
      "24/09/07 10:11:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.6:51068\n",
      "24/09/07 10:11:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.8:49134\n",
      "24/09/07 10:11:29 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 11) (172.18.0.9, executor 2, partition 9, ANY, 7702 bytes) \n",
      "24/09/07 10:12:25 INFO StandaloneSchedulerBackend: Requesting to kill executor(s) 5\n",
      "24/09/07 10:12:25 INFO StandaloneSchedulerBackend: Actual list of executor(s) to be killed is 5\n",
      "24/09/07 10:12:25 INFO ExecutorAllocationManager: Executors 5 removed due to idle timeout.\n",
      "24/09/07 10:12:25 INFO StandaloneSchedulerBackend: Requesting to kill executor(s) 6\n",
      "24/09/07 10:12:25 INFO StandaloneSchedulerBackend: Actual list of executor(s) to be killed is 6\n",
      "24/09/07 10:12:25 INFO ExecutorAllocationManager: Executors 6 removed due to idle timeout.\n",
      "24/09/07 10:12:26 INFO StandaloneSchedulerBackend: Requesting to kill executor(s) 9\n",
      "24/09/07 10:12:26 INFO StandaloneSchedulerBackend: Actual list of executor(s) to be killed is 9\n",
      "24/09/07 10:12:26 INFO ExecutorAllocationManager: Executors 9 removed due to idle timeout.\n",
      "24/09/07 10:12:30 INFO TaskSchedulerImpl: Executor 5 on 172.18.0.4 killed by driver.\n",
      "24/09/07 10:12:30 INFO DAGScheduler: Executor lost: 5 (epoch 1)\n",
      "24/09/07 10:12:30 INFO ExecutorMonitor: Executor 5 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 1, unexpectedly exited: 0).\n",
      "24/09/07 10:12:30 INFO BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.\n",
      "24/09/07 10:12:30 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(5, 172.18.0.4, 40423, None)\n",
      "24/09/07 10:12:30 INFO BlockManagerMaster: Removed 5 successfully in removeExecutor\n",
      "24/09/07 10:12:30 INFO DAGScheduler: Shuffle files lost for executor: 5 (epoch 1)\n",
      "24/09/07 10:12:31 INFO TaskSchedulerImpl: Executor 6 on 172.18.0.9 killed by driver.\n",
      "24/09/07 10:12:31 INFO DAGScheduler: Executor lost: 6 (epoch 2)\n",
      "24/09/07 10:12:31 INFO ExecutorMonitor: Executor 6 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 2, unexpectedly exited: 0).\n",
      "24/09/07 10:12:31 INFO BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.\n",
      "24/09/07 10:12:31 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(6, 172.18.0.9, 45057, None)\n",
      "24/09/07 10:12:31 INFO BlockManagerMaster: Removed 6 successfully in removeExecutor\n",
      "24/09/07 10:12:31 INFO DAGScheduler: Shuffle files lost for executor: 6 (epoch 2)\n",
      "24/09/07 10:12:32 INFO TaskSchedulerImpl: Executor 9 on 172.18.0.7 killed by driver.\n",
      "24/09/07 10:12:32 INFO DAGScheduler: Executor lost: 9 (epoch 3)\n",
      "24/09/07 10:12:32 INFO ExecutorMonitor: Executor 9 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 3, unexpectedly exited: 0).\n",
      "24/09/07 10:12:32 INFO BlockManagerMasterEndpoint: Trying to remove executor 9 from BlockManagerMaster.\n",
      "24/09/07 10:12:32 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(9, 172.18.0.7, 41809, None)\n",
      "24/09/07 10:12:32 INFO BlockManagerMaster: Removed 9 successfully in removeExecutor\n",
      "24/09/07 10:12:32 INFO DAGScheduler: Shuffle files lost for executor: 9 (epoch 3)\n",
      "24/09/07 10:13:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 117614 ms on 172.18.0.4 (executor 1) (1/3)\n",
      "24/09/07 10:13:28 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 125245 ms on 172.18.0.8 (executor 2) (2/3)\n",
      "24/09/07 10:13:53 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 154434 ms on 172.18.0.7 (executor 0) (3/3)\n",
      "24/09/07 10:13:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool default\n",
      "24/09/07 10:13:53 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 154.481 s\n",
      "24/09/07 10:13:53 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/07 10:13:53 INFO DAGScheduler: running: Set()\n",
      "24/09/07 10:13:53 INFO DAGScheduler: waiting: Set()\n",
      "24/09/07 10:13:53 INFO DAGScheduler: failed: Set()\n",
      "24/09/07 10:13:53 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 35522958, minimum partition size: 1048576\n",
      "24/09/07 10:13:53 INFO CodeGenerator: Code generated in 11.702583 ms\n",
      "24/09/07 10:13:53 INFO DAGScheduler: Registering RDD 23 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "24/09/07 10:13:53 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "24/09/07 10:13:53 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/09/07 10:13:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "24/09/07 10:13:53 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/07 10:13:53 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/07 10:13:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 80.1 KiB, free 434.0 MiB)\n",
      "24/09/07 10:13:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.9 MiB)\n",
      "24/09/07 10:13:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fc5436c4b308:33387 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/09/07 10:13:53 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/07 10:13:53 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/09/07 10:13:53 INFO TaskSchedulerImpl: Adding task set 4.0 with 3 tasks resource profile 0\n",
      "24/09/07 10:13:53 INFO FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool default\n",
      "24/09/07 10:13:53 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (172.18.0.4, executor 1, partition 0, NODE_LOCAL, 7779 bytes) \n",
      "24/09/07 10:13:53 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 6) (172.18.0.8, executor 2, partition 1, NODE_LOCAL, 7779 bytes) \n",
      "24/09/07 10:13:53 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 7) (172.18.0.7, executor 0, partition 2, NODE_LOCAL, 7779 bytes) \n",
      "24/09/07 10:13:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.8:40219 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:13:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.7:36067 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:13:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.4:43323 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:13:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.8:52128\n",
      "24/09/07 10:13:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.4:34222\n",
      "24/09/07 10:13:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.7:35198\n",
      "24/09/07 10:13:55 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 7) in 1361 ms on 172.18.0.7 (executor 0) (1/3)\n",
      "24/09/07 10:13:55 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 1410 ms on 172.18.0.4 (executor 1) (2/3)\n",
      "24/09/07 10:13:55 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 6) in 1418 ms on 172.18.0.8 (executor 2) (3/3)\n",
      "24/09/07 10:13:55 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool default\n",
      "24/09/07 10:13:55 INFO DAGScheduler: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 1.433 s\n",
      "24/09/07 10:13:55 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/07 10:13:55 INFO DAGScheduler: running: Set()\n",
      "24/09/07 10:13:55 INFO DAGScheduler: waiting: Set()\n",
      "24/09/07 10:13:55 INFO DAGScheduler: failed: Set()\n",
      "24/09/07 10:13:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/07 10:13:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/07 10:13:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "24/09/07 10:13:55 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/09/07 10:13:55 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/09/07 10:13:55 INFO DAGScheduler: Final stage: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/09/07 10:13:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "24/09/07 10:13:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/07 10:13:55 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[25] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/07 10:13:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 213.7 KiB, free 433.7 MiB)\n",
      "24/09/07 10:13:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.6 MiB)\n",
      "24/09/07 10:13:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fc5436c4b308:33387 (size: 77.2 KiB, free: 434.2 MiB)\n",
      "24/09/07 10:13:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/07 10:13:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/07 10:13:55 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/09/07 10:13:55 INFO FairSchedulableBuilder: Added task set TaskSet_8.0 tasks to pool default\n",
      "24/09/07 10:13:55 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (172.18.0.4, executor 1, partition 0, NODE_LOCAL, 7790 bytes) \n",
      "24/09/07 10:13:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.4:43323 (size: 77.2 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:13:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.4:34222\n",
      "24/09/07 10:13:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 912 ms on 172.18.0.4 (executor 1) (1/1)\n",
      "24/09/07 10:13:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool default\n",
      "24/09/07 10:13:56 INFO DAGScheduler: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.953 s\n",
      "24/09/07 10:13:56 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/07 10:13:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/09/07 10:13:56 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.963293 s\n",
      "24/09/07 10:13:56 INFO FileFormatWriter: Start to commit write Job 4a154065-c1e3-4d74-a7e9-93f59f213f3b.\n",
      "24/09/07 10:13:56 INFO FileFormatWriter: Write Job 4a154065-c1e3-4d74-a7e9-93f59f213f3b committed. Elapsed time: 20 ms.\n",
      "24/09/07 10:13:56 INFO FileFormatWriter: Finished processing stats for write job 4a154065-c1e3-4d74-a7e9-93f59f213f3b.\n",
      "24/09/07 10:13:56 INFO BlockManagerInfo: Removed broadcast_1_python on 172.18.0.4:43323 on disk (size: 139.0 B)\n",
      "24/09/07 10:13:56 INFO BlockManagerInfo: Removed broadcast_1_python on 172.18.0.8:40219 on disk (size: 139.0 B)\n",
      "24/09/07 10:13:56 INFO BlockManagerInfo: Removed broadcast_1_python on 172.18.0.7:36067 on disk (size: 139.0 B)\n",
      "24/09/07 10:13:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.4:43323 in memory (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:13:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.7:36067 in memory (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:13:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.8:40219 in memory (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/07 10:13:56 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "24/09/07 10:13:56 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/09/07 10:13:56 INFO SparkUI: Stopped Spark web UI at http://fc5436c4b308:4041\n",
      "24/09/07 10:13:56 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/09/07 10:13:56 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/09/07 10:13:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/09/07 10:13:56 INFO MemoryStore: MemoryStore cleared\n",
      "24/09/07 10:13:56 INFO BlockManager: BlockManager stopped\n",
      "24/09/07 10:13:56 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/09/07 10:13:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/09/07 10:13:56 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/09/07 10:13:56 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/09/07 10:13:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-9446eae2-e740-4899-9418-9355cd2f54a5\n",
      "24/09/07 10:13:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-9446eae2-e740-4899-9418-9355cd2f54a5/pyspark-773592a3-7c02-4fb4-9a14-13c9d1285528\n",
      "24/09/07 10:13:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-0e11dce6-68ee-4234-a198-31b2ce9c7acf\n",
      "24/09/07 10:15:01 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 9) in 215489 ms on 172.18.0.6 (executor 8) (5/10)\n",
      "24/09/07 10:16:28 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 10) in 302258 ms on 172.18.0.8 (executor 7) (6/10)\n",
      "24/09/07 10:19:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 470494 ms on 172.18.0.8 (executor 0) (7/10)\n",
      "24/09/07 10:19:12 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 3) in 473178 ms on 172.18.0.6 (executor 1) (8/10)\n",
      "24/09/07 10:19:16 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 472283 ms on 172.18.0.6 (executor 3) (9/10)\n",
      "24/09/07 10:19:20 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 11) in 471162 ms on 172.18.0.9 (executor 2) (10/10)\n",
      "24/09/07 10:19:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool default\n",
      "24/09/07 10:19:20 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 481.767 s\n",
      "24/09/07 10:19:20 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/07 10:19:20 INFO DAGScheduler: running: Set()\n",
      "24/09/07 10:19:20 INFO DAGScheduler: waiting: Set()\n",
      "24/09/07 10:19:20 INFO DAGScheduler: failed: Set()\n",
      "24/09/07 10:19:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/07 10:19:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/07 10:19:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "24/09/07 10:19:21 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/09/07 10:19:21 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/09/07 10:19:21 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/09/07 10:19:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "24/09/07 10:19:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/07 10:19:21 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/07 10:19:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 212.7 KiB, free 433.9 MiB)\n",
      "24/09/07 10:19:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.8 MiB)\n",
      "24/09/07 10:19:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fc5436c4b308:36115 (size: 76.9 KiB, free: 434.3 MiB)\n",
      "24/09/07 10:19:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/07 10:19:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/07 10:19:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/09/07 10:19:21 INFO FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool default\n",
      "24/09/07 10:19:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12) (172.18.0.6, executor 3, partition 0, PROCESS_LOCAL, 7619 bytes) \n",
      "24/09/07 10:19:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.6:35773 (size: 76.9 KiB, free: 2.2 GiB)\n",
      "24/09/07 10:19:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.6:51056\n",
      "24/09/07 10:19:26 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 4818 ms on 172.18.0.6 (executor 3) (1/1)\n",
      "24/09/07 10:19:26 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool default\n",
      "24/09/07 10:19:26 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 4.923 s\n",
      "24/09/07 10:19:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/07 10:19:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/09/07 10:19:26 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 4.953233 s\n",
      "24/09/07 10:19:26 INFO FileFormatWriter: Start to commit write Job d4babd67-60f3-4d02-81b5-60b191cedc4c.\n",
      "24/09/07 10:19:26 INFO FileFormatWriter: Write Job d4babd67-60f3-4d02-81b5-60b191cedc4c committed. Elapsed time: 210 ms.\n",
      "24/09/07 10:19:26 INFO FileFormatWriter: Finished processing stats for write job d4babd67-60f3-4d02-81b5-60b191cedc4c.\n",
      "24/09/07 10:19:27 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "24/09/07 10:19:27 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/09/07 10:19:27 INFO SparkUI: Stopped Spark web UI at http://fc5436c4b308:4040\n",
      "24/09/07 10:19:27 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/09/07 10:19:27 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/09/07 10:19:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/09/07 10:19:27 INFO MemoryStore: MemoryStore cleared\n",
      "24/09/07 10:19:27 INFO BlockManager: BlockManager stopped\n",
      "24/09/07 10:19:27 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/09/07 10:19:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/09/07 10:19:27 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/09/07 10:19:27 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/09/07 10:19:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d6ba0a2-776d-49a6-a89d-ffacffd304b2/pyspark-dedf126e-b9b7-40df-9af0-54c872c01621\n",
      "24/09/07 10:19:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-09df4034-0551-43d1-b734-a8150f51b0b1\n",
      "24/09/07 10:19:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d6ba0a2-776d-49a6-a89d-ffacffd304b2\n"
     ]
    }
   ],
   "source": [
    "# for wp in sorted(os.listdir(\"warc_paths\")):\n",
    "#     # remove exist_ok arg in the actual run\n",
    "#     process_wp(wp)\n",
    "#     # break\n",
    "\n",
    "wp = \"warc_2024.paths\"\n",
    "process_wp(wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ca08f-06a8-458e-aa41-ffd6bdd969e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
