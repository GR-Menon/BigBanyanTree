{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6626fae3-de0a-48db-9225-6462a6a2e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import subprocess\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af95ef7-55a3-4dd5-b4ed-249fd6566f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"warc_paths\", exist_ok=True)\n",
    "with open(\"yearly_crawls.txt\", 'r') as f:\n",
    "    for cc_crawl in f:\n",
    "        year = cc_crawl.split('-')[-2]\n",
    "        file_name = f\"./warc_paths/warc_{year}.paths.gz\"\n",
    "        urlretrieve(cc_crawl, file_name)\n",
    "        os.system(f\"gzip -d {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd9a9e9-b0b3-4710-b3a6-05e08b9e170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_warcs_to_proc(wp_file: str) -> int:\n",
    "    \"\"\"Returns the number of lines in the warc.paths file.\"\"\"\n",
    "    with open(f\"./warc_paths/{wp_file}\", 'r') as f:\n",
    "        for count,_ in enumerate(f):\n",
    "            pass\n",
    "    return count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a712fc-0cfc-4fcc-bb93-0e7c604ccdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_file_splits(wp_file: str):\n",
    "    \"\"\"Given a warc.paths file, generates `.txt` files having specified number of WARC filepaths\"\"\"\n",
    "    warc_sample_len = num_warcs_to_proc(wp_file) // 10\n",
    "    os.system(f\"./file_split.sh warc_paths/{wp_file} warc_splits/ {warc_sample_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410a7b8e-c71b-4bf5-b9b5-a61b90304e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paths(input_txt):\n",
    "    \"\"\"Converts the WARC URLs to their corresponding paths on the device.\"\"\"\n",
    "    updated = []\n",
    "    with open(input_txt, 'r') as f:\n",
    "        for l in f:\n",
    "            l = l.split('/')[-1]\n",
    "            updated.append(\"/opt/workspace/datasets/common_crawl/\" + '.'.join(l.split('.')[:-1]))\n",
    "\n",
    "    with open(input_txt, 'w') as f:\n",
    "        for l in updated:\n",
    "            f.write(l + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5aa8b0f-a747-4a15-bd54-bd99d4d8b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(input_txt: str):\n",
    "    \"\"\"Submits two spark jobs and waits for them to finish.\"\"\"\n",
    "    cmd1 = [\"spark-submit\", \"ipwarc_mmdb_pdudf.py\", \"--input_file\", f\"warc_splits/{input_txt}\", \"--output_dir\", \"ipmaxmind_out\"]\n",
    "    # cmd2 = [\"spark-submit\", \"script_extraction.py\", \"--input_file\", f\"warc_splits/{input_txt}\", \"--output_dir\", \"script_extraction_out\"]\n",
    "\n",
    "    process1 = subprocess.Popen(cmd1)\n",
    "    # process2 = subprocess.Popen(cmd2)\n",
    "\n",
    "    process1.wait()\n",
    "    # process2.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9336162b-2511-4aa2-88b5-a9031f77e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wp(wp_file: str):\n",
    "    \"\"\"Process a warc.paths file by generating splits, and submitting each of the split `.txt` file to spark.\"\"\"\n",
    "    os.makedirs(\"warc_splits\", exist_ok=True)\n",
    "    gen_file_splits(wp)\n",
    "    \n",
    "    ckpt_dir = pathlib.Path(\"warc_splits/.ipynb_checkpoints/\")\n",
    "    if ckpt_dir.exists() and ckpt_dir.is_dir():\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "    data_dir = \"/opt/workspace/datasets/common_crawl/\"\n",
    "    for input_txt in sorted(os.listdir(\"warc_splits\")):\n",
    "        os.makedirs(data_dir)\n",
    "        os.system(f\"./get_files.sh warc_splits/{input_txt} {data_dir}\")\n",
    "        to_paths(f\"warc_splits/{input_txt}\")\n",
    "        submit_job(input_txt)\n",
    "        shutil.rmtree(data_dir)\n",
    "        break\n",
    "        \n",
    "    shutil.rmtree(\"./warc_splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c990e1a5-5516-4f89-916e-d31fa7a31637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File split completed. Files saved in warc_splits/\n",
      "Downloading files from file: warc_splits/warc_part_001.txt ...\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131204131716-00026-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "785M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131204131716-00026-ip-10-33-133-15.ec2.internal.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131204131731-00087-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "623M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131204131731-00087-ip-10-33-133-15.ec2.internal.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131204131732-00075-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "610M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131204131732-00075-ip-10-33-133-15.ec2.internal.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131204131733-00074-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "607M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131204131733-00074-ip-10-33-133-15.ec2.internal.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131204131735-00051-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "616M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131204131735-00051-ip-10-33-133-15.ec2.internal.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131204133234-00047-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "612M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131204133234-00047-ip-10-33-133-15.ec2.internal.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131204133329-00048-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "618M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131204133329-00048-ip-10-33-133-15.ec2.internal.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131204133359-00004-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "617M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131204133359-00004-ip-10-33-133-15.ec2.internal.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131218054925-00087-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "806M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131218054925-00087-ip-10-33-133-15.ec2.internal.warc.gz\n",
      "Processing /opt/workspace/datasets/common_crawl//CC-MAIN-20131218054934-00052-ip-10-33-133-15.ec2.internal.warc.gz ...\n",
      "File size before extraction:\n",
      "865M\t/opt/workspace/datasets/common_crawl//CC-MAIN-20131218054934-00052-ip-10-33-133-15.ec2.internal.warc.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/05 12:42:21 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/09/05 12:42:21 INFO SparkContext: OS info Linux, 6.8.0-39-generic, amd64\n",
      "24/09/05 12:42:21 INFO SparkContext: Java version 11.0.24\n",
      "24/09/05 12:42:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/05 12:42:21 INFO ResourceUtils: ==============================================================\n",
      "24/09/05 12:42:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/05 12:42:21 INFO ResourceUtils: ==============================================================\n",
      "24/09/05 12:42:21 INFO SparkContext: Submitted application: maxmind-warc\n",
      "24/09/05 12:42:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/05 12:42:21 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "24/09/05 12:42:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/05 12:42:21 INFO SecurityManager: Changing view acls to: root\n",
      "24/09/05 12:42:21 INFO SecurityManager: Changing modify acls to: root\n",
      "24/09/05 12:42:21 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/05 12:42:21 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/05 12:42:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "24/09/05 12:42:21 INFO Utils: Successfully started service 'sparkDriver' on port 46301.\n",
      "24/09/05 12:42:21 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/05 12:42:21 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/05 12:42:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/05 12:42:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/05 12:42:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/05 12:42:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-42263bea-c49b-4e37-9903-232a5989c30c\n",
      "24/09/05 12:42:21 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "24/09/05 12:42:21 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/05 12:42:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/05 12:42:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/05 12:42:21 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "24/09/05 12:42:21 INFO FairSchedulableBuilder: Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1\n",
      "24/09/05 12:42:21 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "24/09/05 12:42:21 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 18 ms (0 ms spent in bootstraps)\n",
      "24/09/05 12:42:21 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240905124221-0056\n",
      "24/09/05 12:42:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40265.\n",
      "24/09/05 12:42:21 INFO NettyBlockTransferService: Server created on fc5436c4b308:40265\n",
      "24/09/05 12:42:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/05 12:42:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fc5436c4b308, 40265, None)\n",
      "24/09/05 12:42:21 INFO BlockManagerMasterEndpoint: Registering block manager fc5436c4b308:40265 with 434.4 MiB RAM, BlockManagerId(driver, fc5436c4b308, 40265, None)\n",
      "24/09/05 12:42:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fc5436c4b308, 40265, None)\n",
      "24/09/05 12:42:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fc5436c4b308, 40265, None)\n",
      "24/09/05 12:42:21 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/spark-events/app-20240905124221-0056.inprogress\n",
      "24/09/05 12:42:22 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "24/09/05 12:42:22 INFO Utils: Using initial executors = 2, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "24/09/05 12:42:22 INFO ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
      "24/09/05 12:42:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240905124221-0056/0 on worker-20240901052944-172.18.0.7-40861 (172.18.0.7:40861) with 1 core(s)\n",
      "24/09/05 12:42:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20240905124221-0056/0 on hostPort 172.18.0.7:40861 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/05 12:42:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240905124221-0056/1 on worker-20240901052943-172.18.0.4-41363 (172.18.0.4:41363) with 1 core(s)\n",
      "24/09/05 12:42:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20240905124221-0056/1 on hostPort 172.18.0.4:41363 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/05 12:42:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "24/09/05 12:42:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240905124221-0056/0 is now RUNNING\n",
      "24/09/05 12:42:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240905124221-0056/1 is now RUNNING\n",
      "24/09/05 12:42:22 INFO SparkContext: Added file ./ip_utils.py at spark://fc5436c4b308:46301/files/ip_utils.py with timestamp 1725540142169\n",
      "24/09/05 12:42:22 INFO Utils: Copying /opt/workspace/warc_yearly/ip_utils.py to /tmp/spark-2ebf42ae-f7ec-4742-8e78-d22bb561f4fe/userFiles-44803473-2291-4984-8085-3144c8f6a0a4/ip_utils.py\n",
      "24/09/05 12:42:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/09/05 12:42:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/09/05 12:42:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fc5436c4b308:40265 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/09/05 12:42:22 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/09/05 12:42:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/05 12:42:22 INFO SharedState: Warehouse path is 'file:/opt/workspace/warc_yearly/spark-warehouse'.\n",
      "24/09/05 12:42:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:43296) with ID 0,  ResourceProfileId 0\n",
      "24/09/05 12:42:23 INFO ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "24/09/05 12:42:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:59030) with ID 1,  ResourceProfileId 0\n",
      "24/09/05 12:42:23 INFO ExecutorMonitor: New executor 1 has registered (new total is 2)\n",
      "24/09/05 12:42:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:36883 with 2.2 GiB RAM, BlockManagerId(0, 172.18.0.7, 36883, None)\n",
      "24/09/05 12:42:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 208.0 B, free 434.2 MiB)\n",
      "24/09/05 12:42:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 334.0 B, free 434.2 MiB)\n",
      "24/09/05 12:42:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fc5436c4b308:40265 (size: 334.0 B, free: 434.4 MiB)\n",
      "24/09/05 12:42:23 INFO SparkContext: Created broadcast 1 from broadcast at NativeMethodAccessorImpl.java:0\n",
      "24/09/05 12:42:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:45513 with 2.2 GiB RAM, BlockManagerId(1, 172.18.0.4, 45513, None)\n",
      "24/09/05 12:42:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/09/05 12:42:24 INFO CodeGenerator: Code generated in 151.529193 ms\n",
      "24/09/05 12:42:24 INFO CodeGenerator: Code generated in 18.710955 ms\n",
      "24/09/05 12:42:24 INFO CodeGenerator: Code generated in 5.039673 ms\n",
      "24/09/05 12:42:24 INFO FileInputFormat: Total input files to process : 1\n",
      "24/09/05 12:42:24 INFO DAGScheduler: Registering RDD 3 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/09/05 12:42:24 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/09/05 12:42:24 INFO DAGScheduler: Got map stage job 0 (csv at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "24/09/05 12:42:24 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/09/05 12:42:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "24/09/05 12:42:24 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "24/09/05 12:42:24 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/05 12:42:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.3 KiB, free 434.1 MiB)\n",
      "24/09/05 12:42:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
      "24/09/05 12:42:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fc5436c4b308:40265 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/09/05 12:42:24 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/05 12:42:24 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/05 12:42:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/09/05 12:42:24 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool default\n",
      "24/09/05 12:42:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 1, partition 0, PROCESS_LOCAL, 7856 bytes) \n",
      "24/09/05 12:42:24 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.7, executor 0, partition 1, PROCESS_LOCAL, 7856 bytes) \n",
      "24/09/05 12:42:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.7:36883 (size: 5.5 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.4:45513 (size: 5.5 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.7:36883 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.4:45513 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:25 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 981 ms on 172.18.0.7 (executor 0) (1/2)\n",
      "24/09/05 12:42:25 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 41581\n",
      "24/09/05 12:42:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1122 ms on 172.18.0.4 (executor 1) (2/2)\n",
      "24/09/05 12:42:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool default\n",
      "24/09/05 12:42:25 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 1.145 s\n",
      "24/09/05 12:42:25 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/05 12:42:25 INFO DAGScheduler: running: Set()\n",
      "24/09/05 12:42:25 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1)\n",
      "24/09/05 12:42:25 INFO DAGScheduler: failed: Set()\n",
      "24/09/05 12:42:25 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/05 12:42:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 74.1 KiB, free 434.1 MiB)\n",
      "24/09/05 12:42:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 29.8 KiB, free 434.0 MiB)\n",
      "24/09/05 12:42:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fc5436c4b308:40265 (size: 29.8 KiB, free: 434.3 MiB)\n",
      "24/09/05 12:42:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/05 12:42:25 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/09/05 12:42:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0\n",
      "24/09/05 12:42:25 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool default\n",
      "24/09/05 12:42:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.18.0.4, executor 1, partition 0, NODE_LOCAL, 7873 bytes) \n",
      "24/09/05 12:42:25 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (172.18.0.7, executor 0, partition 1, NODE_LOCAL, 7873 bytes) \n",
      "24/09/05 12:42:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:36883 (size: 29.8 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.4:45513 (size: 29.8 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.7:43296\n",
      "24/09/05 12:42:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.4:59030\n",
      "24/09/05 12:42:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.7:36883 (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/05 12:42:26 INFO BlockManagerInfo: Added broadcast_1_python on disk on 172.18.0.7:36883 (size: 139.0 B)\n",
      "24/09/05 12:42:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.4:45513 (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/05 12:42:26 INFO BlockManagerInfo: Added broadcast_1_python on disk on 172.18.0.4:45513 (size: 139.0 B)\n",
      "24/09/05 12:42:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240905124221-0056/2 on worker-20240901052944-172.18.0.9-41535 (172.18.0.9:41535) with 1 core(s)\n",
      "24/09/05 12:42:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20240905124221-0056/2 on hostPort 172.18.0.9:41535 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/05 12:42:26 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 3 for resource profile id: 0)\n",
      "24/09/05 12:42:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240905124221-0056/2 is now RUNNING\n",
      "24/09/05 12:42:27 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:47898) with ID 2,  ResourceProfileId 0\n",
      "24/09/05 12:42:27 INFO ExecutorMonitor: New executor 2 has registered (new total is 3)\n",
      "24/09/05 12:42:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:40811 with 2.2 GiB RAM, BlockManagerId(2, 172.18.0.9, 40811, None)\n",
      "24/09/05 12:42:27 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (172.18.0.9, executor 2, partition 2, PROCESS_LOCAL, 7873 bytes) \n",
      "24/09/05 12:42:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.9:40811 (size: 29.8 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.9:47898\n",
      "24/09/05 12:42:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:40811 (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/05 12:42:29 INFO BlockManagerInfo: Added broadcast_1_python on disk on 172.18.0.9:40811 (size: 139.0 B)\n",
      "24/09/05 12:42:29 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 2065 ms on 172.18.0.9 (executor 2) (1/3)\n",
      "24/09/05 12:42:44 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 18647 ms on 172.18.0.7 (executor 0) (2/3)\n",
      "24/09/05 12:42:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 26829 ms on 172.18.0.4 (executor 1) (3/3)\n",
      "24/09/05 12:42:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool default\n",
      "24/09/05 12:42:52 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 26.846 s\n",
      "24/09/05 12:42:52 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/05 12:42:52 INFO DAGScheduler: running: Set()\n",
      "24/09/05 12:42:52 INFO DAGScheduler: waiting: Set()\n",
      "24/09/05 12:42:52 INFO DAGScheduler: failed: Set()\n",
      "24/09/05 12:42:52 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 4973330, minimum partition size: 1048576\n",
      "24/09/05 12:42:52 INFO CodeGenerator: Code generated in 6.190535 ms\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Registering RDD 23 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/05 12:42:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 80.1 KiB, free 434.0 MiB)\n",
      "24/09/05 12:42:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.9 MiB)\n",
      "24/09/05 12:42:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fc5436c4b308:40265 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/09/05 12:42:52 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/09/05 12:42:52 INFO TaskSchedulerImpl: Adding task set 4.0 with 3 tasks resource profile 0\n",
      "24/09/05 12:42:52 INFO FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool default\n",
      "24/09/05 12:42:52 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (172.18.0.4, executor 1, partition 0, NODE_LOCAL, 7779 bytes) \n",
      "24/09/05 12:42:52 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 6) (172.18.0.7, executor 0, partition 1, NODE_LOCAL, 7779 bytes) \n",
      "24/09/05 12:42:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.4:45513 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.7:36883 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.4:59030\n",
      "24/09/05 12:42:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.7:43296\n",
      "24/09/05 12:42:52 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 7) (172.18.0.4, executor 1, partition 2, NODE_LOCAL, 7779 bytes) \n",
      "24/09/05 12:42:52 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 307 ms on 172.18.0.4 (executor 1) (1/3)\n",
      "24/09/05 12:42:52 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 6) in 364 ms on 172.18.0.7 (executor 0) (2/3)\n",
      "24/09/05 12:42:52 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 7) in 125 ms on 172.18.0.4 (executor 1) (3/3)\n",
      "24/09/05 12:42:52 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool default\n",
      "24/09/05 12:42:52 INFO DAGScheduler: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 0.439 s\n",
      "24/09/05 12:42:52 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/05 12:42:52 INFO DAGScheduler: running: Set()\n",
      "24/09/05 12:42:52 INFO DAGScheduler: waiting: Set()\n",
      "24/09/05 12:42:52 INFO DAGScheduler: failed: Set()\n",
      "24/09/05 12:42:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/09/05 12:42:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/09/05 12:42:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "24/09/05 12:42:52 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Final stage: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[25] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/05 12:42:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 213.6 KiB, free 433.7 MiB)\n",
      "24/09/05 12:42:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.3 KiB, free 433.6 MiB)\n",
      "24/09/05 12:42:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fc5436c4b308:40265 (size: 77.3 KiB, free: 434.2 MiB)\n",
      "24/09/05 12:42:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/05 12:42:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/05 12:42:52 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/09/05 12:42:52 INFO FairSchedulableBuilder: Added task set TaskSet_8.0 tasks to pool default\n",
      "24/09/05 12:42:52 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (172.18.0.4, executor 1, partition 0, NODE_LOCAL, 7790 bytes) \n",
      "24/09/05 12:42:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.4:45513 (size: 77.3 KiB, free: 2.2 GiB)\n",
      "24/09/05 12:42:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.4:59030\n",
      "24/09/05 12:42:53 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 237 ms on 172.18.0.4 (executor 1) (1/1)\n",
      "24/09/05 12:42:53 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool default\n",
      "24/09/05 12:42:53 INFO DAGScheduler: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.259 s\n",
      "24/09/05 12:42:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/05 12:42:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/09/05 12:42:53 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.265700 s\n",
      "24/09/05 12:42:53 INFO FileFormatWriter: Start to commit write Job 2c396946-d2ff-4064-aead-c918027685de.\n",
      "24/09/05 12:42:53 INFO FileFormatWriter: Write Job 2c396946-d2ff-4064-aead-c918027685de committed. Elapsed time: 11 ms.\n",
      "24/09/05 12:42:53 INFO FileFormatWriter: Finished processing stats for write job 2c396946-d2ff-4064-aead-c918027685de.\n",
      "24/09/05 12:42:53 INFO BlockManagerInfo: Removed broadcast_1_python on 172.18.0.7:36883 on disk (size: 139.0 B)\n",
      "24/09/05 12:42:53 INFO BlockManagerInfo: Removed broadcast_1_python on 172.18.0.4:45513 on disk (size: 139.0 B)\n",
      "24/09/05 12:42:53 INFO BlockManagerInfo: Removed broadcast_1_python on 172.18.0.9:40811 on disk (size: 139.0 B)\n",
      "24/09/05 12:42:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.7:36883 in memory (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/05 12:42:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.4:45513 in memory (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/05 12:42:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.9:40811 in memory (size: 334.0 B, free: 2.2 GiB)\n",
      "24/09/05 12:42:53 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "24/09/05 12:42:53 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/09/05 12:42:53 INFO SparkUI: Stopped Spark web UI at http://fc5436c4b308:4041\n",
      "24/09/05 12:42:53 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/09/05 12:42:53 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/09/05 12:42:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/09/05 12:42:53 INFO MemoryStore: MemoryStore cleared\n",
      "24/09/05 12:42:53 INFO BlockManager: BlockManager stopped\n",
      "24/09/05 12:42:53 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/09/05 12:42:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/09/05 12:42:53 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/09/05 12:42:53 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/09/05 12:42:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-6ce47d56-3fe3-4248-a9e3-419aa5f39f8d\n",
      "24/09/05 12:42:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-2ebf42ae-f7ec-4742-8e78-d22bb561f4fe/pyspark-a622eae5-97c5-420a-a310-07c8433ceb29\n",
      "24/09/05 12:42:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-2ebf42ae-f7ec-4742-8e78-d22bb561f4fe\n"
     ]
    }
   ],
   "source": [
    "for wp in sorted(os.listdir(\"warc_paths\")):\n",
    "    # remove exist_ok arg in the actual run\n",
    "    process_wp(wp)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720010a-7d5d-4e6a-b8bc-090904dd1754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39822789-fd57-4f45-a57e-d79bda08f8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
