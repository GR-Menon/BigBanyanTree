{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6626fae3-de0a-48db-9225-6462a6a2e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import subprocess\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af95ef7-55a3-4dd5-b4ed-249fd6566f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"warc_paths\", exist_ok=True)\n",
    "with open(\"yearly_crawls.txt\", 'r') as f:\n",
    "    for cc_crawl in f:\n",
    "        year = cc_crawl.split('-')[-2]\n",
    "        file_name = f\"./warc_paths/warc_{year}.paths.gz\"\n",
    "        urlretrieve(cc_crawl, file_name)\n",
    "        os.system(f\"gzip -d {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227b517d-b8c2-4555-8ad9-5d8d39ddfa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./unsuccessful/\", exist_ok=True)\n",
    "os.makedirs(\"./success/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd9a9e9-b0b3-4710-b3a6-05e08b9e170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_warcs_to_proc(wp_file: str) -> int:\n",
    "    \"\"\"Returns the number of lines in the warc.paths file.\"\"\"\n",
    "    with open(f\"./warc_paths/{wp_file}\", 'r') as f:\n",
    "        for count,_ in enumerate(f):\n",
    "            pass\n",
    "    return count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a712fc-0cfc-4fcc-bb93-0e7c604ccdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_file_splits(wp_file: str):\n",
    "    \"\"\"Given a warc.paths file, generates `.txt` files having specified number of WARC filepaths\"\"\"\n",
    "    warc_sample_len = num_warcs_to_proc(wp_file) // 100\n",
    "    os.system(f\"./file_split.sh warc_paths/{wp_file} warc_splits/ {warc_sample_len} {wp_file.split('_')[-1].split('.')[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "410a7b8e-c71b-4bf5-b9b5-a61b90304e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paths(input_txt):\n",
    "    \"\"\"Converts the WARC URLs to their corresponding paths on the device.\"\"\"\n",
    "    updated = []\n",
    "    with open(input_txt, 'r') as f:\n",
    "        for l in f:\n",
    "            l = l.split('/')[-1]\n",
    "            updated.append(\"/opt/workspace/datasets/common_crawl/\" + '.'.join(l.split('.')[:-1]))\n",
    "\n",
    "    with open(input_txt, 'w') as f:\n",
    "        for l in updated:\n",
    "            f.write(l + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5aa8b0f-a747-4a15-bd54-bd99d4d8b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(input_txt: str):\n",
    "    \"\"\"Submits two spark jobs and waits for them to finish. If both jobs succeed, then the `input_txt` file is moved to success/ dir.\"\"\"\n",
    "    os.makedirs(\"tmp/\", exist_ok=True)\n",
    "    cmd1 = [\"spark-submit\", \"ipwarc_mmdb_pdudf-errh.py\", \"--input_file\", f\"warc_splits/{input_txt}\", \"--output_dir\", \"tmp/ipmaxmind_out\"]\n",
    "    cmd2 = [\"spark-submit\", \"script_extraction-errh.py\", \"--input_file\", f\"warc_splits/{input_txt}\", \"--output_dir\", \"tmp/script_extraction_out\"]\n",
    "\n",
    "    status_file = \"job_status.txt\"\n",
    "    if os.path.exists(status_file):\n",
    "        os.remove(status_file)\n",
    "\n",
    "    process1 = subprocess.Popen(cmd1)\n",
    "    process2 = subprocess.Popen(cmd2)\n",
    "\n",
    "    process1.wait()\n",
    "    process2.wait()\n",
    "\n",
    "    with open(status_file, 'r') as f:\n",
    "        statuses = f.readlines()\n",
    "\n",
    "    # Check if both jobs succeeded\n",
    "    if all(\"success\" in status for status in statuses):\n",
    "        \n",
    "        # Move temp output to final directory\n",
    "        for filename in os.listdir(\"tmp/ipmaxmind_out/\"):\n",
    "            if filename == \".ipynb_checkpoints\": continue\n",
    "            src_file = os.path.join(\"tmp/ipmaxmind_out/\", filename)\n",
    "            dst_file = os.path.join(\"ipmaxmind_out/\", filename)\n",
    "            shutil.move(src_file, dst_file)\n",
    "\n",
    "        for filename in os.listdir(\"tmp/script_extraction_out/\"):\n",
    "            if filename == \".ipynb_checkpoints\": continue\n",
    "            src_file = os.path.join(\"tmp/script_extraction_out/\", filename)\n",
    "            dst_file = os.path.join(\"script_extraction_out/\", filename)\n",
    "            shutil.move(src_file, dst_file)\n",
    "            \n",
    "        print(\"Both jobs succeeded. Outputs moved to final directories.\")\n",
    "        \n",
    "        input_dir = os.path.dirname(f\"warc_splits/{input_txt}\")\n",
    "        shutil.move(f\"warc_splits/{input_txt}\", os.path.join(\"success/\", os.path.basename(f\"warc_splits/{input_txt}\")))\n",
    "        \n",
    "        print(f\"Processing completed successfully. Input file warc_splits/{input_txt} moved to success/\")\n",
    "        \n",
    "    else:\n",
    "        # If any job failed, discard temporary output\n",
    "        shutil.rmtree('tmp/ipmaxmind_out', ignore_errors=True)\n",
    "        shutil.rmtree('tmp/script_extraction_out', ignore_errors=True)\n",
    "        print(\"One or more jobs failed. Outputs discarded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9336162b-2511-4aa2-88b5-a9031f77e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wp(wp_file: str):\n",
    "    \"\"\"Process a warc.paths file by generating splits, and submitting each of the split `.txt` file to spark.\"\"\"\n",
    "    start_time = time.time()\n",
    "    os.makedirs(\"warc_splits\", exist_ok=True)\n",
    "    gen_file_splits(wp_file)\n",
    "    \n",
    "    ckpt_dir = pathlib.Path(\"warc_splits/.ipynb_checkpoints/\")\n",
    "    if ckpt_dir.exists() and ckpt_dir.is_dir():\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "    data_dir = \"/opt/workspace/datasets/common_crawl/\"\n",
    "    # data_dir = \"/opt/workspace/warc_yearly/data/\"\n",
    "    for input_txt in sorted(os.listdir(\"warc_splits\")):\n",
    "        if input_txt == \".ipynb_checkpoints\": continue\n",
    "        os.makedirs(data_dir)\n",
    "        os.system(f\"./get_files.sh warc_splits/{input_txt} {data_dir}\")\n",
    "        to_paths(f\"warc_splits/{input_txt}\")\n",
    "        submit_job(input_txt)\n",
    "        shutil.rmtree(data_dir)\n",
    "\n",
    "    # files that are processed successfully are moved to `success/`.\n",
    "    # remaining files are hence not processed successfully.\n",
    "    for file in os.listdir(\"warc_splits\"):\n",
    "        if file == \".ipynb_checkpoints\": continue\n",
    "        shutil.move(f\"warc_splits/{file}\", os.path.join(\"unsuccessful/\", os.path.basename(file)))\n",
    "        \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    with open(\"times.txt\", 'w') as f:\n",
    "        f.write(f\"[{wp_file}]: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c990e1a5-5516-4f89-916e-d31fa7a31637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File split completed. Files saved in warc_splits/\n",
      "Downloading files from file: warc_splits/warc_part_001_2024.txt ...\n",
      "Total files to download: 50\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640353668.0/warc/CC-MAIN-20240802234508-20240803024508-00404.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641085898.84/warc/CC-MAIN-20240813204036-20240813234036-00257.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640694594.35/warc/CC-MAIN-20240807143134-20240807173134-00080.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640997721.66/warc/CC-MAIN-20240811110531-20240811140531-00889.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640723918.41/warc/CC-MAIN-20240808062406-20240808092406-00440.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640447331.19/warc/CC-MAIN-20240805114033-20240805144033-00148.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641079807.82/warc/CC-MAIN-20240813141635-20240813171635-00638.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640365107.3/warc/CC-MAIN-20240803091113-20240803121113-00279.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641113960.89/warc/CC-MAIN-20240814123926-20240814153926-00481.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640492117.28/warc/CC-MAIN-20240806130705-20240806160705-00418.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641002566.67/warc/CC-MAIN-20240811141716-20240811171716-00803.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641028735.71/warc/CC-MAIN-20240812030550-20240812060550-00190.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640694449.36/warc/CC-MAIN-20240807111957-20240807141957-00446.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641052535.77/warc/CC-MAIN-20240812221559-20240813011559-00607.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640393185.10/warc/CC-MAIN-20240804071743-20240804101743-00441.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640728444.43/warc/CC-MAIN-20240808124926-20240808154926-00373.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640353668.0/warc/CC-MAIN-20240802234508-20240803024508-00836.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640726723.42/warc/CC-MAIN-20240808093647-20240808123647-00287.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640713269.38/warc/CC-MAIN-20240807205613-20240807235613-00632.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640789586.56/warc/CC-MAIN-20240810030800-20240810060800-00660.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640380725.7/warc/CC-MAIN-20240803214957-20240804004957-00211.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640713903.39/warc/CC-MAIN-20240808000606-20240808030606-00348.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640451031.21/warc/CC-MAIN-20240805144950-20240805174950-00583.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640728444.43/warc/CC-MAIN-20240808124926-20240808154926-00819.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722641036895.73/warc/CC-MAIN-20240812092946-20240812122946-00514.warc.gz ...\n",
      "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/segments/1722640508059.30/warc/CC-MAIN-20240806192936-20240806222936-00695.warc.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 17:18:31 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/09/10 17:18:31 INFO SparkContext: OS info Linux, 6.8.0-39-generic, amd64\n",
      "24/09/10 17:18:31 INFO SparkContext: Java version 11.0.24\n",
      "24/09/10 17:18:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/10 17:18:31 INFO ResourceUtils: ==============================================================\n",
      "24/09/10 17:18:31 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/10 17:18:31 INFO ResourceUtils: ==============================================================\n",
      "24/09/10 17:18:31 INFO SparkContext: Submitted application: script_extraction\n",
      "24/09/10 17:18:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/10 17:18:31 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "24/09/10 17:18:31 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/10 17:18:31 INFO SecurityManager: Changing view acls to: root\n",
      "24/09/10 17:18:31 INFO SecurityManager: Changing modify acls to: root\n",
      "24/09/10 17:18:31 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/10 17:18:31 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/10 17:18:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "24/09/10 17:18:31 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/09/10 17:18:31 INFO SparkContext: OS info Linux, 6.8.0-39-generic, amd64\n",
      "24/09/10 17:18:31 INFO SparkContext: Java version 11.0.24\n",
      "24/09/10 17:18:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/10 17:18:31 INFO Utils: Successfully started service 'sparkDriver' on port 41777.\n",
      "24/09/10 17:18:31 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/10 17:18:31 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/10 17:18:31 INFO ResourceUtils: ==============================================================\n",
      "24/09/10 17:18:31 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/10 17:18:31 INFO ResourceUtils: ==============================================================\n",
      "24/09/10 17:18:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/10 17:18:31 INFO SparkContext: Submitted application: maxmind-warc\n",
      "24/09/10 17:18:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/10 17:18:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/10 17:18:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/10 17:18:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9b599103-0f6c-4a78-8f4e-039e973f2ccf\n",
      "24/09/10 17:18:31 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "24/09/10 17:18:31 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/10 17:18:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "24/09/10 17:18:31 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/10 17:18:31 INFO SecurityManager: Changing view acls to: root\n",
      "24/09/10 17:18:31 INFO SecurityManager: Changing modify acls to: root\n",
      "24/09/10 17:18:31 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/10 17:18:31 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/10 17:18:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "24/09/10 17:18:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/10 17:18:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/09/10 17:18:31 INFO FairSchedulableBuilder: Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1\n",
      "24/09/10 17:18:31 INFO Utils: Successfully started service 'sparkDriver' on port 43495.\n",
      "24/09/10 17:18:31 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "24/09/10 17:18:31 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/10 17:18:31 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 19 ms (0 ms spent in bootstraps)\n",
      "24/09/10 17:18:31 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/10 17:18:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/10 17:18:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/10 17:18:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/10 17:18:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f9422f66-dd2d-4576-8cc0-e4f1f3577a4d\n",
      "24/09/10 17:18:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "24/09/10 17:18:31 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240910171831-0101\n",
      "24/09/10 17:18:31 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/10 17:18:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44553.\n",
      "24/09/10 17:18:31 INFO NettyBlockTransferService: Server created on fc5436c4b308:44553\n",
      "24/09/10 17:18:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/10 17:18:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fc5436c4b308, 44553, None)\n",
      "24/09/10 17:18:31 INFO BlockManagerMasterEndpoint: Registering block manager fc5436c4b308:44553 with 434.4 MiB RAM, BlockManagerId(driver, fc5436c4b308, 44553, None)\n",
      "24/09/10 17:18:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fc5436c4b308, 44553, None)\n",
      "24/09/10 17:18:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fc5436c4b308, 44553, None)\n",
      "24/09/10 17:18:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/10 17:18:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/10 17:18:31 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "24/09/10 17:18:32 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/spark-events/app-20240910171831-0101.inprogress\n",
      "24/09/10 17:18:32 INFO FairSchedulableBuilder: Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1\n",
      "24/09/10 17:18:32 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "24/09/10 17:18:32 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 18 ms (0 ms spent in bootstraps)\n",
      "24/09/10 17:18:32 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "24/09/10 17:18:32 INFO Utils: Using initial executors = 2, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "24/09/10 17:18:32 INFO ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
      "24/09/10 17:18:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240910171831-0101/0 on worker-20240901052944-172.18.0.8-46473 (172.18.0.8:46473) with 1 core(s)\n",
      "24/09/10 17:18:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20240910171831-0101/0 on hostPort 172.18.0.8:46473 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/10 17:18:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240910171831-0101/1 on worker-20240901052944-172.18.0.6-44801 (172.18.0.6:44801) with 1 core(s)\n",
      "24/09/10 17:18:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20240910171831-0101/1 on hostPort 172.18.0.6:44801 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/10 17:18:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240910171831-0101/1 is now RUNNING\n",
      "24/09/10 17:18:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240910171831-0101/0 is now RUNNING\n",
      "24/09/10 17:18:32 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "24/09/10 17:18:32 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240910171832-0102\n",
      "24/09/10 17:18:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39115.\n",
      "24/09/10 17:18:32 INFO NettyBlockTransferService: Server created on fc5436c4b308:39115\n",
      "24/09/10 17:18:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/10 17:18:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fc5436c4b308, 39115, None)\n",
      "24/09/10 17:18:32 INFO BlockManagerMasterEndpoint: Registering block manager fc5436c4b308:39115 with 434.4 MiB RAM, BlockManagerId(driver, fc5436c4b308, 39115, None)\n",
      "24/09/10 17:18:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fc5436c4b308, 39115, None)\n",
      "24/09/10 17:18:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fc5436c4b308, 39115, None)\n",
      "24/09/10 17:18:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 208.0 B, free 434.4 MiB)\n",
      "24/09/10 17:18:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 276.0 B, free 434.4 MiB)\n",
      "24/09/10 17:18:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fc5436c4b308:44553 (size: 276.0 B, free: 434.4 MiB)\n",
      "24/09/10 17:18:32 INFO SparkContext: Created broadcast 0 from broadcast at NativeMethodAccessorImpl.java:0\n",
      "24/09/10 17:18:32 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/spark-events/app-20240910171832-0102.inprogress\n",
      "24/09/10 17:18:32 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "24/09/10 17:18:32 INFO Utils: Using initial executors = 2, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "24/09/10 17:18:32 INFO ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
      "24/09/10 17:18:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/09/10 17:18:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240910171832-0102/0 on worker-20240901052944-172.18.0.7-40861 (172.18.0.7:40861) with 1 core(s)\n",
      "24/09/10 17:18:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20240910171832-0102/0 on hostPort 172.18.0.7:40861 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/10 17:18:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240910171832-0102/1 on worker-20240901052943-172.18.0.4-41363 (172.18.0.4:41363) with 1 core(s)\n",
      "24/09/10 17:18:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20240910171832-0102/1 on hostPort 172.18.0.4:41363 with 1 core(s), 4.0 GiB RAM\n",
      "24/09/10 17:18:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/09/10 17:18:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fc5436c4b308:44553 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/09/10 17:18:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240910171832-0102/0 is now RUNNING\n",
      "24/09/10 17:18:32 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/09/10 17:18:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240910171832-0102/1 is now RUNNING\n",
      "24/09/10 17:18:32 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "24/09/10 17:18:32 INFO SparkContext: Added file ./ip_utils.py at spark://fc5436c4b308:43495/files/ip_utils.py with timestamp 1725988712586\n",
      "24/09/10 17:18:32 INFO Utils: Copying /opt/workspace/warc_yearly/ip_utils.py to /tmp/spark-a2c393fa-639c-435d-8f67-d592ffba5d82/userFiles-e7bd7bc3-450f-4c14-b1a9-1f6d44b8e924/ip_utils.py\n",
      "24/09/10 17:18:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 208.0 B, free 434.4 MiB)\n",
      "24/09/10 17:18:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 332.0 B, free 434.4 MiB)\n",
      "24/09/10 17:18:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fc5436c4b308:39115 (size: 332.0 B, free: 434.4 MiB)\n",
      "24/09/10 17:18:32 INFO SparkContext: Created broadcast 0 from broadcast at NativeMethodAccessorImpl.java:0\n",
      "24/09/10 17:18:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/10 17:18:32 INFO SharedState: Warehouse path is 'file:/opt/workspace/warc_yearly/spark-warehouse'.\n",
      "24/09/10 17:18:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/09/10 17:18:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/09/10 17:18:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fc5436c4b308:39115 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/09/10 17:18:33 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/09/10 17:18:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/10 17:18:33 INFO SharedState: Warehouse path is 'file:/opt/workspace/warc_yearly/spark-warehouse'.\n",
      "24/09/10 17:18:34 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:50706) with ID 1,  ResourceProfileId 0\n",
      "24/09/10 17:18:34 INFO ExecutorMonitor: New executor 1 has registered (new total is 1)\n",
      "24/09/10 17:18:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:41839 with 2.2 GiB RAM, BlockManagerId(1, 172.18.0.6, 41839, None)\n",
      "24/09/10 17:18:34 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.8:60460) with ID 0,  ResourceProfileId 0\n",
      "24/09/10 17:18:34 INFO ExecutorMonitor: New executor 0 has registered (new total is 2)\n",
      "24/09/10 17:18:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.8:38297 with 2.2 GiB RAM, BlockManagerId(0, 172.18.0.8, 38297, None)\n",
      "24/09/10 17:18:34 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:39676) with ID 0,  ResourceProfileId 0\n",
      "24/09/10 17:18:34 INFO ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "24/09/10 17:18:34 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:32944) with ID 1,  ResourceProfileId 0\n",
      "24/09/10 17:18:34 INFO ExecutorMonitor: New executor 1 has registered (new total is 2)\n",
      "24/09/10 17:18:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:42669 with 2.2 GiB RAM, BlockManagerId(0, 172.18.0.7, 42669, None)\n",
      "24/09/10 17:18:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:36739 with 2.2 GiB RAM, BlockManagerId(1, 172.18.0.4, 36739, None)\n",
      "24/09/10 17:18:34 INFO CodeGenerator: Code generated in 120.994603 ms\n",
      "24/09/10 17:18:34 INFO FileInputFormat: Total input files to process : 1\n",
      "24/09/10 17:18:34 INFO DAGScheduler: Registering RDD 3 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/09/10 17:18:34 INFO DAGScheduler: Registering RDD 12 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/09/10 17:18:34 INFO DAGScheduler: Got map stage job 0 (parquet at NativeMethodAccessorImpl.java:0) with 10 output partitions\n",
      "24/09/10 17:18:34 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "24/09/10 17:18:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "24/09/10 17:18:34 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "24/09/10 17:18:34 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/10 17:18:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.3 KiB, free 434.1 MiB)\n",
      "24/09/10 17:18:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.1 MiB)\n",
      "24/09/10 17:18:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fc5436c4b308:44553 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "24/09/10 17:18:34 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/10 17:18:34 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/10 17:18:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/09/10 17:18:34 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool default\n",
      "24/09/10 17:18:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 7690 bytes) \n",
      "24/09/10 17:18:34 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7690 bytes) \n",
      "24/09/10 17:18:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/09/10 17:18:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:41839 (size: 5.6 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.8:38297 (size: 5.6 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.8:38297 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:41839 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:35 INFO CodeGenerator: Code generated in 195.037903 ms\n",
      "24/09/10 17:18:35 INFO CodeGenerator: Code generated in 31.096883 ms\n",
      "24/09/10 17:18:35 INFO CodeGenerator: Code generated in 8.400263 ms\n",
      "24/09/10 17:18:35 INFO FileInputFormat: Total input files to process : 1\n",
      "24/09/10 17:18:35 INFO DAGScheduler: Registering RDD 3 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/09/10 17:18:35 INFO DAGScheduler: Registering RDD 19 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/09/10 17:18:35 INFO DAGScheduler: Got map stage job 0 (csv at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "24/09/10 17:18:35 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/09/10 17:18:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "24/09/10 17:18:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "24/09/10 17:18:35 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/10 17:18:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.3 KiB, free 434.1 MiB)\n",
      "24/09/10 17:18:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.1 MiB)\n",
      "24/09/10 17:18:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fc5436c4b308:39115 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "24/09/10 17:18:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/10 17:18:35 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/10 17:18:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/09/10 17:18:35 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool default\n",
      "24/09/10 17:18:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7861 bytes) \n",
      "24/09/10 17:18:35 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.4, executor 1, partition 1, PROCESS_LOCAL, 7861 bytes) \n",
      "24/09/10 17:18:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.7:42669 (size: 5.6 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.4:36739 (size: 5.6 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.7:42669 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.4:36739 (size: 32.6 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:35 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1028 ms on 172.18.0.8 (executor 0) (1/2)\n",
      "24/09/10 17:18:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1036 ms on 172.18.0.6 (executor 1) (2/2)\n",
      "24/09/10 17:18:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool default\n",
      "24/09/10 17:18:35 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 42057\n",
      "24/09/10 17:18:35 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 1.076 s\n",
      "24/09/10 17:18:35 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/10 17:18:35 INFO DAGScheduler: running: Set()\n",
      "24/09/10 17:18:35 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1)\n",
      "24/09/10 17:18:36 INFO DAGScheduler: failed: Set()\n",
      "24/09/10 17:18:36 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/10 17:18:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 33.1 KiB, free 434.1 MiB)\n",
      "24/09/10 17:18:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 434.1 MiB)\n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fc5436c4b308:44553 (size: 16.5 KiB, free: 434.3 MiB)\n",
      "24/09/10 17:18:36 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/10 17:18:36 INFO DAGScheduler: Submitting 10 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "24/09/10 17:18:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 10 tasks resource profile 0\n",
      "24/09/10 17:18:36 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool default\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 2) (172.18.0.8, executor 0, partition 4, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (172.18.0.6, executor 1, partition 0, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.8:38297 (size: 16.5 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.6:41839 (size: 16.5 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:41839 (size: 276.0 B, free: 2.2 GiB)\n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_0_python on disk on 172.18.0.6:41839 (size: 98.0 B)\n",
      "24/09/10 17:18:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.6:50706\n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.8:38297 (size: 276.0 B, free: 2.2 GiB)\n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_0_python on disk on 172.18.0.8:38297 (size: 98.0 B)\n",
      "24/09/10 17:18:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.8:60460\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4) (172.18.0.6, executor 1, partition 1, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 3) (172.18.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 5) (172.18.0.8, executor 0, partition 5, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 WARN TaskSetManager: Lost task 4.0 in stage 1.0 (TID 2) (172.18.0.8 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240815173031-20240815203031-00872.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 6) (172.18.0.6, executor 1, partition 0, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 4) (172.18.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240803214957-20240804004957-00211.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 4.1 in stage 1.0 (TID 7) (172.18.0.8, executor 0, partition 4, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 WARN TaskSetManager: Lost task 5.0 in stage 1.0 (TID 5) (172.18.0.8 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240803060126-20240803090126-00180.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 1.1 in stage 1.0 (TID 8) (172.18.0.6, executor 1, partition 1, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Lost task 0.1 in stage 1.0 (TID 6) on 172.18.0.6, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      ") [duplicate 1]\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1108 ms on 172.18.0.4 (executor 1) (1/2)\n",
      "24/09/10 17:18:36 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 33631\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 5.1 in stage 1.0 (TID 9) (172.18.0.8, executor 0, partition 5, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Lost task 4.1 in stage 1.0 (TID 7) on 172.18.0.8, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240815173031-20240815203031-00872.warc'\n",
      ") [duplicate 1]\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1137 ms on 172.18.0.7 (executor 0) (2/2)\n",
      "24/09/10 17:18:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool default\n",
      "24/09/10 17:18:36 INFO DAGScheduler: ShuffleMapStage 0 (coalesce at NativeMethodAccessorImpl.java:0) finished in 1.164 s\n",
      "24/09/10 17:18:36 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/10 17:18:36 INFO DAGScheduler: running: Set()\n",
      "24/09/10 17:18:36 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1)\n",
      "24/09/10 17:18:36 INFO DAGScheduler: failed: Set()\n",
      "24/09/10 17:18:36 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/09/10 17:18:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 74.1 KiB, free 434.1 MiB)\n",
      "24/09/10 17:18:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 29.8 KiB, free 434.0 MiB)\n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fc5436c4b308:39115 (size: 29.8 KiB, free: 434.3 MiB)\n",
      "24/09/10 17:18:36 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/09/10 17:18:36 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/09/10 17:18:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0\n",
      "24/09/10 17:18:36 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool default\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.18.0.7, executor 0, partition 0, NODE_LOCAL, 7873 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (172.18.0.4, executor 1, partition 1, NODE_LOCAL, 7873 bytes) \n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:42669 (size: 29.8 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 10) (172.18.0.6, executor 1, partition 0, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Lost task 1.1 in stage 1.0 (TID 8) on 172.18.0.6, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240803214957-20240804004957-00211.warc'\n",
      ") [duplicate 1]\n",
      "24/09/10 17:18:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.4:36739 (size: 29.8 KiB, free: 2.2 GiB)\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 4.2 in stage 1.0 (TID 11) (172.18.0.8, executor 0, partition 4, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Lost task 5.1 in stage 1.0 (TID 9) on 172.18.0.8, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240803060126-20240803090126-00180.warc'\n",
      ") [duplicate 1]\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 1.2 in stage 1.0 (TID 12) (172.18.0.6, executor 1, partition 1, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Lost task 0.2 in stage 1.0 (TID 10) on 172.18.0.6, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      ") [duplicate 2]\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 5.2 in stage 1.0 (TID 13) (172.18.0.8, executor 0, partition 5, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Lost task 4.2 in stage 1.0 (TID 11) on 172.18.0.8, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240815173031-20240815203031-00872.warc'\n",
      ") [duplicate 2]\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 0.3 in stage 1.0 (TID 14) (172.18.0.6, executor 1, partition 0, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Lost task 1.2 in stage 1.0 (TID 12) on 172.18.0.6, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240803214957-20240804004957-00211.warc'\n",
      ") [duplicate 2]\n",
      "24/09/10 17:18:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.7:39676\n",
      "24/09/10 17:18:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.4:32944\n",
      "24/09/10 17:18:36 INFO TaskSetManager: Starting task 4.3 in stage 1.0 (TID 15) (172.18.0.8, executor 0, partition 4, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:36 INFO TaskSetManager: Lost task 5.2 in stage 1.0 (TID 13) on 172.18.0.8, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240803060126-20240803090126-00180.warc'\n",
      ") [duplicate 2]\n",
      "24/09/10 17:18:37 INFO TaskSetManager: Starting task 1.3 in stage 1.0 (TID 16) (172.18.0.6, executor 1, partition 1, NODE_LOCAL, 7702 bytes) \n",
      "24/09/10 17:18:37 INFO TaskSetManager: Lost task 0.3 in stage 1.0 (TID 14) on 172.18.0.6, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      ") [duplicate 3]\n",
      "24/09/10 17:18:37 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job\n",
      "24/09/10 17:18:37 INFO TaskSchedulerImpl: Cancelling stage 1\n",
      "24/09/10 17:18:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 14) (172.18.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/09/10 17:18:37 INFO TaskSchedulerImpl: Stage 1 was cancelled\n",
      "24/09/10 17:18:37 INFO DAGScheduler: ShuffleMapStage 1 (parquet at NativeMethodAccessorImpl.java:0) failed in 1.036 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 14) (172.18.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/09/10 17:18:37 WARN TaskSetManager: Lost task 4.3 in stage 1.0 (TID 15) (172.18.0.8 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 14) (172.18.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/10 17:18:37 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/09/10 17:18:37 INFO SparkUI: Stopped Spark web UI at http://fc5436c4b308:4040\n",
      "24/09/10 17:18:37 WARN TaskSetManager: Lost task 1.3 in stage 1.0 (TID 16) (172.18.0.6 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 14) (172.18.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/10 17:18:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool default\n",
      "24/09/10 17:18:37 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/09/10 17:18:37 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/09/10 17:18:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/09/10 17:18:37 INFO MemoryStore: MemoryStore cleared\n",
      "24/09/10 17:18:37 INFO BlockManager: BlockManager stopped\n",
      "24/09/10 17:18:37 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/09/10 17:18:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/09/10 17:18:37 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/09/10 17:18:37 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/09/10 17:18:37 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "24/09/10 17:18:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-0708a968-698b-44b4-92c4-40d54a291856/pyspark-e7867c7f-d089-4d50-94b1-d25b7e978006\n",
      "24/09/10 17:18:37 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/09/10 17:18:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-c31a5a64-d71c-43d8-9fd0-fe320f588a52\n",
      "24/09/10 17:18:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-0708a968-698b-44b4-92c4-40d54a291856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 117, in <module>\n",
      "    df.repartition(1).write.mode(\"append\").parquet(args.output_dir)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1721, in parquet\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o82.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 14) (172.18.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 94, in proc_wrapper\n",
      "    for res in process_warc(filepath):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 85, in process_warc\n",
      "    with open(filepath, 'rb') as stream:\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/workspace/datasets/common_crawl/CC-MAIN-20240811141716-20240811171716-00803.warc'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 130, in <module>\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/workspace/warc_yearly/script_extraction-errh.py\", line 138, in <module>\n",
      "    spark.stop()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 1796, in stop\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py\", line 666, in stop\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 316, in shutdown\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 253, in shutdown\n",
      "    self.__is_shut_down.wait()\n",
      "  File \"/usr/local/lib/python3.11/threading.py\", line 629, in wait\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=3>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "    signaled = self._cond.wait(timeout)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/threading.py\", line 327, in wait\n",
      "    waiter.acquire()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py\", line 381, in signal_handler\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "AttributeError: 'NoneType' object has no attribute 'sc'\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o33.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 17:18:37 INFO SparkUI: Stopped Spark web UI at http://fc5436c4b308:4041\n",
      "24/09/10 17:18:37 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) failed in 0.644 s due to Stage cancelled because SparkContext was shut down\n",
      "24/09/10 17:18:37 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/09/10 17:18:37 INFO SparkContext: SparkContext already stopped.\n",
      "24/09/10 17:18:37 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/09/10 17:18:37 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/09/10 17:18:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/09/10 17:18:37 INFO MemoryStore: MemoryStore cleared\n",
      "24/09/10 17:18:37 INFO BlockManager: BlockManager stopped\n",
      "24/09/10 17:18:37 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/09/10 17:18:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/09/10 17:18:37 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/09/10 17:18:37 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/09/10 17:18:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2c393fa-639c-435d-8f67-d592ffba5d82/pyspark-88ce591d-ac3c-4e08-89a3-3a1a02584d9e\n",
      "24/09/10 17:18:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-c580b1ba-a702-4f80-9049-4a62c2f9c2e0\n",
      "24/09/10 17:18:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2c393fa-639c-435d-8f67-d592ffba5d82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for wp in sorted(os.listdir(\"warc_paths\")):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     # remove exist_ok arg in the actual run\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     process_wp(wp)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     # break\u001b[39;00m\n\u001b[1;32m      6\u001b[0m wp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarc_2024.paths\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mprocess_wp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mprocess_wp\u001b[0;34m(wp_file)\u001b[0m\n\u001b[1;32m     16\u001b[0m     os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./get_files.sh warc_splits/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_txt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     to_paths(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarc_splits/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_txt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     \u001b[43msubmit_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_txt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(data_dir)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# files that are processed successfully are moved to `success/`.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# remaining files are hence not processed successfully.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36msubmit_job\u001b[0;34m(input_txt)\u001b[0m\n\u001b[1;32m     11\u001b[0m process1 \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd1)\n\u001b[1;32m     12\u001b[0m process2 \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd2)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mprocess1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m process2\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(status_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/subprocess.py:2053\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 2053\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/subprocess.py:2011\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2011\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   2013\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   2015\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for wp in sorted(os.listdir(\"warc_paths\")):\n",
    "#     # remove exist_ok arg in the actual run\n",
    "#     process_wp(wp)\n",
    "#     # break\n",
    "\n",
    "wp = \"warc_2024.paths\"\n",
    "process_wp(wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de358922-ab95-4ea1-8608-d5504f4b48cd",
   "metadata": {},
   "source": [
    "### note: the input.txt file is moved to success/ even if only 1 script runs successfully on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394bbf0-66a1-4756-be03-90e2df7dee65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
